{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5539168,"sourceType":"datasetVersion","datasetId":3192482},{"sourceId":9920101,"sourceType":"datasetVersion","datasetId":6096635},{"sourceId":10005539,"sourceType":"datasetVersion","datasetId":6159058},{"sourceId":11565412,"sourceType":"datasetVersion","datasetId":7251391},{"sourceId":11668498,"sourceType":"datasetVersion","datasetId":7322954},{"sourceId":11768589,"sourceType":"datasetVersion","datasetId":7388291},{"sourceId":11943480,"sourceType":"datasetVersion","datasetId":7486175},{"sourceId":388325,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":309912,"modelId":330281},{"sourceId":406790,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":330058,"modelId":350912},{"sourceId":412460,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":330058,"modelId":350912}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/Liu-Tianchi/MFA-TDNN.git \n!pip install speechbrain \"typeguard<3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:05:09.368546Z","iopub.execute_input":"2025-05-31T03:05:09.368810Z","iopub.status.idle":"2025-05-31T03:06:23.025102Z","shell.execute_reply.started":"2025-05-31T03:05:09.368789Z","shell.execute_reply":"2025-05-31T03:06:23.024041Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'MFA-TDNN'...\nremote: Enumerating objects: 51, done.\u001b[K\nremote: Counting objects: 100% (51/51), done.\u001b[K\nremote: Compressing objects: 100% (47/47), done.\u001b[K\nremote: Total 51 (delta 22), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (51/51), 18.93 KiB | 4.73 MiB/s, done.\nResolving deltas: 100% (22/22), done.\nCollecting speechbrain\n  Downloading speechbrain-1.0.3-py3-none-any.whl.metadata (24 kB)\nCollecting typeguard<3\n  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\nCollecting hyperpyyaml (from speechbrain)\n  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.4.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain) (24.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.15.2)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.2.0)\nRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.5.1+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.5.1+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speechbrain) (4.67.1)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.30.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.9->speechbrain)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.9->speechbrain)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.9->speechbrain)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.9->speechbrain)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.9->speechbrain)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.9->speechbrain)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.9->speechbrain)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->speechbrain) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->speechbrain) (1.3.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (2.32.3)\nCollecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n  Downloading ruamel.yaml-0.18.12-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->speechbrain) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->speechbrain) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->speechbrain) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->speechbrain) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->speechbrain) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->speechbrain) (2.4.1)\nCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->speechbrain) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->speechbrain) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->speechbrain) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->speechbrain) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->speechbrain) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->speechbrain) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->speechbrain) (2024.2.0)\nDownloading speechbrain-1.0.3-py3-none-any.whl (864 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m864.1/864.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\nDownloading ruamel.yaml-0.18.12-py3-none-any.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.4/118.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: typeguard, ruamel.yaml.clib, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, ruamel.yaml, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, hyperpyyaml, speechbrain\n  Attempting uninstall: typeguard\n    Found existing installation: typeguard 4.4.1\n    Uninstalling typeguard-4.4.1:\n      Successfully uninstalled typeguard-4.4.1\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.16.1 requires typeguard<5,>=3, but you have typeguard 2.13.3 which is incompatible.\ninflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed hyperpyyaml-1.2.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ruamel.yaml-0.18.12 ruamel.yaml.clib-0.2.12 speechbrain-1.0.3 typeguard-2.13.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/working/MFA-TDNN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:06:25.467756Z","iopub.execute_input":"2025-05-31T03:06:25.468044Z","iopub.status.idle":"2025-05-31T03:06:25.472142Z","shell.execute_reply.started":"2025-05-31T03:06:25.468020Z","shell.execute_reply":"2025-05-31T03:06:25.471520Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom ECAPA_tc_0813 import ECAPA_tc_0813, Classifier \nmodel = ECAPA_tc_0813(input_size=192)\nx = torch.rand(2, 200, 80)\nprint(model(x).squeeze(1).shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:06:27.367659Z","iopub.execute_input":"2025-05-31T03:06:27.367905Z","iopub.status.idle":"2025-05-31T03:06:33.105936Z","shell.execute_reply.started":"2025-05-31T03:06:27.367888Z","shell.execute_reply":"2025-05-31T03:06:33.105118Z"}},"outputs":[{"name":"stdout","text":"160 640 4\n1 640 512\n2 512 512\n3 512 512\ntorch.Size([2, 192])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## ECAPA Architecture","metadata":{}},{"cell_type":"code","source":"'''\nThis is the ECAPA-TDNN model.\nThis model is modified and combined based on the following three projects:\n  1. https://github.com/clovaai/voxceleb_trainer/issues/86\n  2. https://github.com/lawlict/ECAPA-TDNN/blob/master/ecapa_tdnn.py\n  3. https://github.com/speechbrain/speechbrain/blob/96077e9a1afff89d3f5ff47cab4bca0202770e4f/speechbrain/lobes/models/ECAPA_TDNN.py\n\n'''\n\nimport math, torch, torchaudio\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SEModule(nn.Module):\n    def __init__(self, channels, bottleneck=128):\n        super(SEModule, self).__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool1d(1),\n            nn.Conv1d(channels, bottleneck, kernel_size=1, padding=0),\n            nn.ReLU(),\n            # nn.BatchNorm1d(bottleneck), # I remove this layer\n            nn.Conv1d(bottleneck, channels, kernel_size=1, padding=0),\n            nn.Sigmoid(),\n            )\n\n    def forward(self, input):\n        x = self.se(input)\n        return input * x\n\nclass Bottle2neck(nn.Module):\n\n    def __init__(self, inplanes, planes, kernel_size=None, dilation=None, scale = 8):\n        super(Bottle2neck, self).__init__()\n        width       = int(math.floor(planes / scale))\n        self.conv1  = nn.Conv1d(inplanes, width*scale, kernel_size=1)\n        self.bn1    = nn.BatchNorm1d(width*scale)\n        self.nums   = scale -1\n        convs       = []\n        bns         = []\n        num_pad = math.floor(kernel_size/2)*dilation\n        for i in range(self.nums):\n            convs.append(nn.Conv1d(width, width, kernel_size=kernel_size, dilation=dilation, padding=num_pad))\n            bns.append(nn.BatchNorm1d(width))\n        self.convs  = nn.ModuleList(convs)\n        self.bns    = nn.ModuleList(bns)\n        self.conv3  = nn.Conv1d(width*scale, planes, kernel_size=1)\n        self.bn3    = nn.BatchNorm1d(planes)\n        self.relu   = nn.ReLU()\n        self.width  = width\n        self.se     = SEModule(planes)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.bn1(out)\n\n        spx = torch.split(out, self.width, 1)\n        for i in range(self.nums):\n          if i==0:\n            sp = spx[i]\n          else:\n            sp = sp + spx[i]\n          sp = self.convs[i](sp)\n          sp = self.relu(sp)\n          sp = self.bns[i](sp)\n          if i==0:\n            out = sp\n          else:\n            out = torch.cat((out, sp), 1)\n        out = torch.cat((out, spx[self.nums]),1)\n\n        out = self.conv3(out)\n        out = self.relu(out)\n        out = self.bn3(out)\n        \n        out = self.se(out)\n        out += residual\n        return out \n\nclass PreEmphasis(torch.nn.Module):\n\n    def __init__(self, coef: float = 0.97):\n        super().__init__()\n        self.coef = coef\n        self.register_buffer(\n            'flipped_filter', torch.FloatTensor([-self.coef, 1.]).unsqueeze(0).unsqueeze(0)\n        )\n\n    def forward(self, input: torch.tensor) -> torch.tensor:\n        input = input.unsqueeze(1)\n        input = F.pad(input, (1, 0), 'reflect')\n        return F.conv1d(input, self.flipped_filter).squeeze(1)\n\nclass FbankAug(nn.Module):\n\n    def __init__(self, freq_mask_width = (0, 8), time_mask_width = (0, 10)):\n        self.time_mask_width = time_mask_width\n        self.freq_mask_width = freq_mask_width\n        super().__init__()\n\n    def mask_along_axis(self, x, dim):\n        original_size = x.shape\n        batch, fea, time = x.shape\n        if dim == 1:\n            D = fea\n            width_range = self.freq_mask_width\n        else:\n            D = time\n            width_range = self.time_mask_width\n\n        mask_len = torch.randint(width_range[0], width_range[1], (batch, 1), device=x.device).unsqueeze(2)\n        mask_pos = torch.randint(0, max(1, D - mask_len.max()), (batch, 1), device=x.device).unsqueeze(2)\n        arange = torch.arange(D, device=x.device).view(1, 1, -1)\n        mask = (mask_pos <= arange) * (arange < (mask_pos + mask_len))\n        mask = mask.any(dim=1)\n\n        if dim == 1:\n            mask = mask.unsqueeze(2)\n        else:\n            mask = mask.unsqueeze(1)\n            \n        x = x.masked_fill_(mask, 0.0)\n        return x.view(*original_size)\n\n    def forward(self, x):    \n        x = self.mask_along_axis(x, dim=2)\n        x = self.mask_along_axis(x, dim=1)\n        return x\n\nclass ECAPA_TDNN(nn.Module):\n\n    def __init__(self, C=512):\n\n        super(ECAPA_TDNN, self).__init__()\n\n        self.torchfbank = torch.nn.Sequential(\n            PreEmphasis(),            \n            torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=512, win_length=400, hop_length=160, \\\n                                                 f_min = 20, f_max = 7600, window_fn=torch.hamming_window, n_mels=80),\n            )\n\n        self.specaug = FbankAug() # Spec augmentation\n\n        self.conv1  = nn.Conv1d(80, C, kernel_size=5, stride=1, padding=2)\n        self.relu   = nn.ReLU()\n        self.bn1    = nn.BatchNorm1d(C)\n        self.layer1 = Bottle2neck(C, C, kernel_size=3, dilation=1, scale=8)\n        self.extra_conv1 = nn.Conv1d(C, int(C/4), kernel_size=1)\n        self.layer2 = Bottle2neck(C, C, kernel_size=3, dilation=2, scale=8)\n        self.extra_conv2 = nn.Conv1d(C, int(C/4), kernel_size=1)\n        self.layer3 = Bottle2neck(C, C, kernel_size=3, dilation=2, scale=8)\n        self.extra_conv3 = nn.Conv1d(C, int(C/2), kernel_size=1)\n        self.extra_layer1 = Bottle2neck(C, C, kernel_size=3, dilation=3, scale=8)\n        self.extra_layer2 = Bottle2neck(C, C, kernel_size=3, dilation=4, scale=8)\n        # I fixed the shape of the output from MFA layer, that is close to the setting from ECAPA paper.\n        self.layer4 = nn.Conv1d(3*C, 1536, kernel_size=1)\n        self.attention = nn.Sequential(\n            nn.Conv1d(4608, 256, kernel_size=1),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Tanh(), # I add this layer\n            nn.Conv1d(256, 1536, kernel_size=1),\n            nn.Softmax(dim=2),\n            )\n        self.bn5 = nn.BatchNorm1d(3072)\n        self.fc6 = nn.Linear(3072, 192)\n        self.bn6 = nn.BatchNorm1d(192)\n\n\n    def forward(self, x, aug):\n        with torch.no_grad():\n            x = self.torchfbank(x)+1e-6\n            x = x.log()   \n            x = x - torch.mean(x, dim=-1, keepdim=True)\n            if aug == True:\n                x = self.specaug(x)\n\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.bn1(x)\n\n        x1 = self.layer1(x)\n        x2 = self.layer2(x+x1)\n        x3 = self.layer3(x+x1+x2)\n        x4 = self.extra_layer1(x+x1+x2+x3)\n        x5 = self.extra_layer2(x+x1+x2+x3+x4)\n\n        x = self.layer4(torch.cat((self.extra_conv1(x1),self.extra_conv2(x2),self.extra_conv3(x3),x4,x5),dim=1))\n        x = self.relu(x)\n\n        t = x.size()[-1]\n\n        global_x = torch.cat((x,torch.mean(x,dim=2,keepdim=True).repeat(1,1,t), torch.sqrt(torch.var(x,dim=2,keepdim=True).clamp(min=1e-4)).repeat(1,1,t)), dim=1)\n        \n        w = self.attention(global_x)\n\n        mu = torch.sum(x * w, dim=2)\n        sg = torch.sqrt( ( torch.sum((x**2) * w, dim=2) - mu**2 ).clamp(min=1e-4) )\n\n        x = torch.cat((mu,sg),1)\n        x = self.bn5(x)\n        x = self.fc6(x)\n        x = self.bn6(x)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:06:35.973073Z","iopub.execute_input":"2025-05-31T03:06:35.973615Z","iopub.status.idle":"2025-05-31T03:06:35.997950Z","shell.execute_reply.started":"2025-05-31T03:06:35.973583Z","shell.execute_reply":"2025-05-31T03:06:35.997317Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Tools","metadata":{}},{"cell_type":"code","source":"'''\nSome utilized functions\nThese functions are all copied from voxceleb_trainer: https://github.com/clovaai/voxceleb_trainer/blob/master/tuneThreshold.py\n'''\n\nimport os, numpy, torch\nfrom sklearn import metrics\nfrom operator import itemgetter\nimport torch.nn.functional as F\n\ndef init_args(args):\n\targs.score_save_path    = os.path.join(args.save_path, 'score.txt')\n\targs.model_save_path    = os.path.join(args.save_path, 'model')\n\tos.makedirs(args.model_save_path, exist_ok = True)\n\treturn args\n\ndef tuneThresholdfromScore(scores, labels, target_fa, target_fr = None):\n\t\n\tfpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=1)\n\tfnr = 1 - tpr\n\ttunedThreshold = [];\n\tif target_fr:\n\t\tfor tfr in target_fr:\n\t\t\tidx = numpy.nanargmin(numpy.absolute((tfr - fnr)))\n\t\t\ttunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n\tfor tfa in target_fa:\n\t\tidx = numpy.nanargmin(numpy.absolute((tfa - fpr))) # numpy.where(fpr<=tfa)[0][-1]\n\t\ttunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n\tidxE = numpy.nanargmin(numpy.absolute((fnr - fpr)))\n\teer  = max(fpr[idxE],fnr[idxE])*100\n\t\n\treturn tunedThreshold, eer, fpr, fnr\n\n# Creates a list of false-negative rates, a list of false-positive rates\n# and a list of decision thresholds that give those error-rates.\ndef ComputeErrorRates(scores, labels):\n\n      # Sort the scores from smallest to largest, and also get the corresponding\n      # indexes of the sorted scores.  We will treat the sorted scores as the\n      # thresholds at which the the error-rates are evaluated.\n      sorted_indexes, thresholds = zip(*sorted(\n          [(index, threshold) for index, threshold in enumerate(scores)],\n          key=itemgetter(1)))\n      sorted_labels = []\n      labels = [labels[i] for i in sorted_indexes]\n      fnrs = []\n      fprs = []\n\n      # At the end of this loop, fnrs[i] is the number of errors made by\n      # incorrectly rejecting scores less than thresholds[i]. And, fprs[i]\n      # is the total number of times that we have correctly accepted scores\n      # greater than thresholds[i].\n      for i in range(0, len(labels)):\n          if i == 0:\n              fnrs.append(labels[i])\n              fprs.append(1 - labels[i])\n          else:\n              fnrs.append(fnrs[i-1] + labels[i])\n              fprs.append(fprs[i-1] + 1 - labels[i])\n      fnrs_norm = sum(labels)\n      fprs_norm = len(labels) - fnrs_norm\n\n      # Now divide by the total number of false negative errors to\n      # obtain the false positive rates across all thresholds\n      fnrs = [x / float(fnrs_norm) for x in fnrs]\n\n      # Divide by the total number of corret positives to get the\n      # true positive rate.  Subtract these quantities from 1 to\n      # get the false positive rates.\n      fprs = [1 - x / float(fprs_norm) for x in fprs]\n      return fnrs, fprs, thresholds\n\n# Computes the minimum of the detection cost function.  The comments refer to\n# equations in Section 3 of the NIST 2016 Speaker Recognition Evaluation Plan.\ndef ComputeMinDcf(fnrs, fprs, thresholds, p_target, c_miss, c_fa):\n    min_c_det = float(\"inf\")\n    min_c_det_threshold = thresholds[0]\n    for i in range(0, len(fnrs)):\n        # See Equation (2).  it is a weighted sum of false negative\n        # and false positive errors.\n        c_det = c_miss * fnrs[i] * p_target + c_fa * fprs[i] * (1 - p_target)\n        if c_det < min_c_det:\n            min_c_det = c_det\n            min_c_det_threshold = thresholds[i]\n    # See Equations (3) and (4).  Now we normalize the cost.\n    c_def = min(c_miss * p_target, c_fa * (1 - p_target))\n    min_dcf = min_c_det / c_def\n    return min_dcf, min_c_det_threshold\n\ndef accuracy(output, target, topk=(1,)):\n\n\tmaxk = max(topk)\n\tbatch_size = target.size(0)\n\t_, pred = output.topk(maxk, 1, True, True)\n\tpred = pred.t()\n\tcorrect = pred.eq(target.view(1, -1).expand_as(pred))\n\tres = []\n\tfor k in topk:\n\t\tcorrect_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n\t\tres.append(correct_k.mul_(100.0 / batch_size))\n\t\n\treturn res","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:06:39.853960Z","iopub.execute_input":"2025-05-31T03:06:39.854351Z","iopub.status.idle":"2025-05-31T03:06:40.103928Z","shell.execute_reply.started":"2025-05-31T03:06:39.854328Z","shell.execute_reply":"2025-05-31T03:06:40.103376Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## AAM Softmax Loss","metadata":{}},{"cell_type":"code","source":"'''\nAAMsoftmax loss function copied from voxceleb_trainer: https://github.com/clovaai/voxceleb_trainer/blob/master/loss/aamsoftmax.py\n'''\n\nimport torch, math\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass AAMsoftmax(nn.Module):\n    def __init__(self, n_class, m, s):\n        \n        super(AAMsoftmax, self).__init__()\n        self.m = m\n        self.s = s\n        self.weight = torch.nn.Parameter(torch.FloatTensor(n_class, 192), requires_grad=True)\n        self.ce = nn.CrossEntropyLoss()\n        nn.init.xavier_normal_(self.weight, gain=1)\n        self.cos_m = math.cos(self.m)\n        self.sin_m = math.sin(self.m)\n        self.th = math.cos(math.pi - self.m)\n        self.mm = math.sin(math.pi - self.m) * self.m\n\n    def forward(self, x, label=None):\n        \n        cosine = F.linear(F.normalize(x), F.normalize(self.weight))\n        sine = torch.sqrt((1.0 - torch.mul(cosine, cosine)).clamp(0, 1))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        phi = torch.where((cosine - self.th) > 0, phi, cosine - self.mm)\n        one_hot = torch.zeros_like(cosine)\n        one_hot.scatter_(1, label.view(-1, 1), 1)\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output = output * self.s\n        \n        loss = self.ce(output, label)\n        prec1 = accuracy(output.detach(), label.detach(), topk=(1,))[0]\n\n        return loss, prec1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:06:43.259657Z","iopub.execute_input":"2025-05-31T03:06:43.260220Z","iopub.status.idle":"2025-05-31T03:06:43.268617Z","shell.execute_reply.started":"2025-05-31T03:06:43.260194Z","shell.execute_reply":"2025-05-31T03:06:43.267869Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## ECAPA Model","metadata":{}},{"cell_type":"code","source":"'''\nThis part is used to train the speaker model and evaluate the performances\n'''\n\nimport torch, sys, os, tqdm, numpy, soundfile, time, pickle, random\nimport torch.nn as nn\n\nclass ECAPAModel(nn.Module):\n    def __init__(self, lr, lr_decay, C, n_class, m, s, test_step, device='cuda', **kwargs):\n        super(ECAPAModel, self).__init__()\n        \n        # Store device\n        self.device = torch.device(device)\n\n        self.torchfbank = torch.nn.Sequential(\n            PreEmphasis(),            \n            torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=512, win_length=400, hop_length=160, \n                                                 f_min=20, f_max=7600, window_fn=torch.hamming_window, n_mels=80),\n        )\n\n        self.specaug = FbankAug()  # Spec augmentation\n\n        # ECAPA-TDNN - Don't force .cuda() here, let it be moved explicitly\n        self.speaker_encoder = ECAPA_tc_0813(input_size=192)\n        # Classifier\n        self.speaker_loss = AAMsoftmax(n_class=n_class, m=m, s=s)\n        \n        # Move model to device after creation\n        self.to(self.device)\n        \n        self.optim = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=2e-5)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optim, step_size=test_step, gamma=lr_decay)\n        \n        print(time.strftime(\"%m-%d %H:%M:%S\") + \" Model para number = %.2f\" %\n              (sum(param.numel() for param in self.speaker_encoder.parameters()) / 1024 / 1024))\n\n    def transform(self, x, aug):\n        with torch.no_grad():\n            x = self.torchfbank(x) + 1e-6\n            x = x.log()\n            x = x - torch.mean(x, dim=-1, keepdim=True)\n            if aug == True:\n                x = self.specaug(x)\n        return x.transpose(1, 2)\n\n    def train_network(self, epoch, loader):\n        self.train()\n        \n        # Update the learning rate based on the current epoch\n        self.scheduler.step(epoch - 1)\n        \n        index, top1, loss = 0, 0, 0\n        lr = self.optim.param_groups[0]['lr']\n        \n        for num, (data, labels) in enumerate(loader, start=1):\n            # Move data to device efficiently\n            if isinstance(data, torch.Tensor):\n                data = data.to(self.device, non_blocking=True)\n            else:\n                data = torch.FloatTensor(data).to(self.device, non_blocking=True)\n            \n            if isinstance(labels, torch.Tensor):\n                labels = labels.to(self.device, non_blocking=True)\n            else:\n                labels = torch.LongTensor(labels).to(self.device, non_blocking=True)\n            \n            # Forward pass\n            self.zero_grad()\n            speaker_embedding = self.speaker_encoder.forward(self.transform(data, aug=True)).squeeze(1)\n            nloss, prec = self.speaker_loss.forward(speaker_embedding, labels)\n            \n            # Backward pass\n            nloss.backward()\n            self.optim.step()\n            \n            # Statistics\n            batch_size = len(labels)\n            index += batch_size\n            top1 += prec\n            loss += nloss.detach().cpu().item()\n            \n            # Progress reporting\n            sys.stderr.write(time.strftime(\"%m-%d %H:%M:%S\") +\n                             \" [%2d] Lr: %5f, Training: %.2f%%, \" % (epoch, lr, 100 * (num / len(loader))) +\n                             \" Loss: %.5f, ACC: %2.2f%% \\r\" % (loss/num, top1/index*100))\n            sys.stderr.flush()\n        \n        sys.stdout.write(\"\\n\")\n        return loss/num, lr, top1/index*100\n\n    def eval_network(self, eval_list, eval_path):\n        self.eval()\n        files = []\n        embeddings = {}\n        lines = open(eval_list).read().splitlines()\n        new_lines = []\n        for line in lines:\n            line = line.strip('\"')\n            if os.path.exists(os.path.join(eval_path, line.split()[1])) and os.path.exists(os.path.join(eval_path, line.split()[2])):\n                files.append(line.split()[1])\n                files.append(line.split()[2])\n                new_lines.append(line)\n        lines = new_lines\n        setfiles = list(set(files))\n        setfiles.sort()\n\n        for idx, file in tqdm.tqdm(enumerate(setfiles), total=len(setfiles)):\n            audio, _ = soundfile.read(os.path.join(eval_path, file))\n            data_1 = torch.FloatTensor(numpy.stack([audio], axis=0)).cuda()\n\n            max_audio = 300 * 160 + 240\n            if audio.shape[0] <= max_audio:\n                shortage = max_audio - audio.shape[0]\n                audio = numpy.pad(audio, (0, shortage), 'wrap')\n            feats = []\n            startframe = numpy.linspace(0, audio.shape[0]-max_audio, num=5)\n            for asf in startframe:\n                feats.append(audio[int(asf):int(asf)+max_audio])\n            feats = numpy.stack(feats, axis=0).astype(float)\n            data_2 = torch.FloatTensor(feats).cuda()\n            with torch.no_grad():\n                embedding_1 = self.speaker_encoder.forward(self.transform(data_1, aug=False)).squeeze(1)\n                embedding_1 = F.normalize(embedding_1, p=2, dim=1)\n                embedding_2 = self.speaker_encoder.forward(self.transform(data_2, aug=False)).squeeze(1)\n                embedding_2 = F.normalize(embedding_2, p=2, dim=1)\n            embeddings[file] = [embedding_1, embedding_2]\n\n        scores, labels = [], []\n        for line in lines:\n            embedding_11, embedding_12 = embeddings[line.split()[1]]\n            embedding_21, embedding_22 = embeddings[line.split()[2]]\n            score_1 = torch.mean(torch.matmul(embedding_11, embedding_21.T))\n            score_2 = torch.mean(torch.matmul(embedding_12, embedding_22.T))\n            score = (score_1 + score_2) / 2\n            score = score.detach().cpu().numpy()\n            scores.append(score)\n            labels.append(int(line.split()[0]))\n\n        EER = tuneThresholdfromScore(scores, labels, [1, 0.1])[1]\n        fnrs, fprs, thresholds = ComputeErrorRates(scores, labels)\n        minDCF, _ = ComputeMinDcf(fnrs, fprs, thresholds, 0.05, 1, 1)\n        \n        return EER, minDCF\n\n    def test_network(self, test_list, test_path):\n        self.eval()\n        files = []\n        scores = []\n        embeddings = {}\n        lines = open(test_list).read().splitlines()\n        for line in lines:\n            files.append(line.split()[0])\n            files.append(line.split()[1])\n        setfiles = list(set(files))\n        setfiles.sort()\n\n        for idx, file in tqdm.tqdm(enumerate(setfiles), total=len(setfiles)):\n            audio, _ = soundfile.read(os.path.join(test_path, file))\n            data_1 = torch.FloatTensor(numpy.stack([audio], axis=0)).cuda()\n\n            max_audio = 300 * 160 + 240\n            if audio.shape[0] <= max_audio:\n                shortage = max_audio - audio.shape[0]\n                audio = numpy.pad(audio, (0, shortage), 'wrap')\n            feats = []\n            startframe = numpy.linspace(0, audio.shape[0]-max_audio, num=5)\n            for asf in startframe:\n                feats.append(audio[int(asf):int(asf)+max_audio])\n            feats = numpy.stack(feats, axis=0).astype(float)\n            data_2 = torch.FloatTensor(feats).cuda()\n            with torch.no_grad():\n                embedding_1 = self.speaker_encoder.forward(self.transform(data_1, aug=False)).squeeze(1)\n                embedding_1 = F.normalize(embedding_1, p=2, dim=1)\n                embedding_2 = self.speaker_encoder.forward(self.transform(data_2, aug=False)).squeeze(1)\n                embedding_2 = F.normalize(embedding_2, p=2, dim=1)\n            embeddings[file] = [embedding_1, embedding_2]\n\n        with open(\"predictions.txt\", \"w\") as f:\n            for line in lines:\n                embedding_11, embedding_12 = embeddings[line.split()[0]]\n                embedding_21, embedding_22 = embeddings[line.split()[1]]\n                score_1 = torch.mean(torch.matmul(embedding_11, embedding_21.T))\n                score_2 = torch.mean(torch.matmul(embedding_12, embedding_22.T))\n                score = (score_1 + score_2) / 2\n                score = score.detach().cpu().numpy()\n                scores.append(score)\n                f.write(f\"{score:.4f}\\n\")\n        return scores\n\n    def forward(self, file_path):\n        audio, _ = soundfile.read(file_path)\n        data = torch.FloatTensor(numpy.stack([audio], axis=0)).cuda()\n        embedding = self.speaker_encoder.forward(self.transform(data, aug=False)).squeeze(1)\n        embedding = F.normalize(embedding, p=2, dim=1)\n        return embedding\n\n    def save_parameters(self, path):\n        torch.save(self.state_dict(), path)\n\n    def load_parameters(self, path):\n        self_state = self.state_dict()\n        loaded_state = torch.load(path)\n        for name, param in loaded_state.items():\n            origname = name\n            if name not in self_state:\n                name = name.replace(\"module.\", \"\")\n                if name not in self_state:\n                    print(\"%s is not in the model.\" % origname)\n                    continue\n            if self_state[name].size() != loaded_state[origname].size():\n                print(\"Wrong parameter length: %s, model: %s, loaded: %s\" % (\n                    origname, self_state[name].size(), loaded_state[origname].size()))\n                continue\n            self_state[name].copy_(param)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:12:36.942147Z","iopub.execute_input":"2025-05-31T03:12:36.942962Z","iopub.status.idle":"2025-05-31T03:12:36.970324Z","shell.execute_reply.started":"2025-05-31T03:12:36.942937Z","shell.execute_reply":"2025-05-31T03:12:36.969758Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Datasets & Data Collator","metadata":{}},{"cell_type":"markdown","source":"### MFCC","metadata":{}},{"cell_type":"code","source":"import librosa\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Mel_Spectrogram(nn.Module):\n    def __init__(self, sample_rate=16000, n_fft=512, win_length=400, hop=160, n_mels=80, coef=0.97, requires_grad=False):\n        super(Mel_Spectrogram, self).__init__()\n        self.n_fft = n_fft\n        self.n_mels = n_mels\n        self.win_length = win_length\n        self.hop = hop\n\n        self.pre_emphasis = PreEmphasis(coef)\n        mel_basis = librosa.filters.mel(\n            sr=sample_rate, n_fft=n_fft, n_mels=n_mels)\n        self.mel_basis = nn.Parameter(\n            torch.FloatTensor(mel_basis), requires_grad=requires_grad)\n        self.instance_norm = nn.InstanceNorm1d(num_features=n_mels)\n        window = torch.hamming_window(self.win_length)\n        self.window = nn.Parameter(\n            torch.FloatTensor(window), requires_grad=False)\n\n    def forward(self, x):\n        x = self.pre_emphasis(x)\n        x = torch.stft(x, n_fft=self.n_fft, hop_length=self.hop,\n                       window=self.window, win_length=self.win_length, return_complex=True)\n        x = torch.abs(x)\n        x += 1e-9\n        x = torch.log(x)\n        x = torch.matmul(self.mel_basis, x)\n        x = self.instance_norm(x)\n        x = x.unsqueeze(1)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:07:18.829618Z","iopub.execute_input":"2025-05-31T03:07:18.830100Z","iopub.status.idle":"2025-05-31T03:07:18.842560Z","shell.execute_reply.started":"2025-05-31T03:07:18.830073Z","shell.execute_reply":"2025-05-31T03:07:18.841851Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport random\n\n\ndef generate_speaker_pairs(csv_file, output_file = \"eval_pairs.txt\", num_pairs=10000):\n    # Load the dataset\n    data = pd.read_csv(csv_file)\n    \n    # Group audio files by speaker ID\n    speakers = data.groupby('speaker')['audio_name'].apply(list).to_dict()\n    all_speakers = list(speakers.keys())\n    pairs = []\n    \n    # Generate positive pairs (same speaker)\n    while len(pairs) < num_pairs // 2:\n        speaker = random.choice(all_speakers)\n        if len(speakers[speaker]) < 2:\n            continue\n        file1, file2 = random.sample(speakers[speaker], 2)\n        pairs.append([1, file1, file2])\n    \n    # Generate negative pairs (different speakers)\n    while len(pairs) < num_pairs:\n        sp1, sp2 = random.sample(all_speakers, 2)\n        file1 = random.choice(speakers[sp1])\n        file2 = random.choice(speakers[sp2])\n        pairs.append([0, file1, file2])\n    \n    with open(output_file, \"w\") as fout:\n        for pair in pairs:\n            fout.write(f\"{pair[0]} {pair[1]} {pair[2]}\\n\")\ntest_path = \"/kaggle/input/vispeech/metadata/noisy_testset.csv\"\ngenerate_speaker_pairs(test_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:07:22.344706Z","iopub.execute_input":"2025-05-31T03:07:22.345287Z","iopub.status.idle":"2025-05-31T03:07:22.665678Z","shell.execute_reply.started":"2025-05-31T03:07:22.345264Z","shell.execute_reply":"2025-05-31T03:07:22.665093Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport pandas as pd\n\nfrom scipy.io import wavfile\nfrom scipy import signal\nimport soundfile\n\ndef compute_dB(waveform):\n    \"\"\"\n    Args:\n        x (numpy.array): Input waveform (#length).\n    Returns:\n        numpy.array: Output array (#length).\n    \"\"\"\n    val = max(0.0, np.mean(np.power(waveform, 2)))\n    dB = 10*np.log10(val+1e-4)\n    return dB\n\nclass WavAugment(object):\n    def __init__(self, noise_csv_path=\"/kaggle/input/csv-files/musan_filelist.csv\", rir_csv_path=\"/kaggle/input/csv-files/rir_data.csv\"):\n        noise_csv = pd.read_csv(\"/kaggle/input/csv-files/musan_filelist.csv\")\n        self.noise_paths = noise_csv[\"utt_paths\"].values\n        self.noise_names = noise_csv[\"speaker_name\"].values\n        self.rir_paths = pd.read_csv(rir_csv_path)[\"utt_paths\"].values\n\n    def augment(self, waveform):\n        if np.random.rand() < 0.4:\n            idx = np.random.randint(0, 8)\n            if idx == 0:\n                waveform = self.add_gaussian_noise(waveform)\n                waveform = self.add_real_noise(waveform)\n    \n            if idx == 1 or idx == 2 or idx == 3:\n                waveform = self.add_real_noise(waveform)\n    \n            if idx == 4 or idx == 5 or idx == 6:\n                waveform = self.reverberate(waveform)\n    \n            if idx == 7:\n                waveform = self.change_volum(waveform)\n                waveform = self.reverberate(waveform)\n    \n            if idx == 6:\n                waveform = self.change_volum(waveform)\n                waveform = self.add_real_noise(waveform)\n    \n            if idx == 8:\n                waveform = self.add_gaussian_noise(waveform)\n                waveform = self.reverberate(waveform)\n\n        return waveform\n\n    def add_gaussian_noise(self, waveform):\n        \"\"\"\n        Args:\n            x (numpy.array): Input waveform array (#length).\n        Returns:\n            numpy.array: Output waveform array (#length).\n        \"\"\"\n        snr = np.random.uniform(low=10, high=25)\n        clean_dB = compute_dB(waveform)\n        noise = np.random.randn(len(waveform))\n        noise_dB = compute_dB(noise)\n        noise = np.sqrt(10 ** ((clean_dB - noise_dB - snr) / 10)) * noise\n        waveform = (waveform + noise)\n        return waveform\n\n    def change_volum(self, waveform):\n        \"\"\"\n        Args:\n            x (numpy.array): Input waveform array (#length).\n        Returns:\n            numpy.array: Output waveform array (#length).\n        \"\"\"\n        volum = np.random.uniform(low=0.8, high=1.0005)\n        waveform = waveform * volum\n        return waveform\n\n    def add_real_noise(self, waveform):\n        \"\"\"\n        Args:\n            x (numpy.array): Input length (#length).\n        Returns:\n            numpy.array: Output waveform array (#length).\n        \"\"\"\n        clean_dB = compute_dB(waveform)\n\n        idx = np.random.randint(0, len(self.noise_paths))\n        sample_rate, noise = wavfile.read(self.noise_paths[idx])\n        noise = noise.astype(np.float64)\n\n        snr = np.random.uniform(15, 25)\n\n        noise_length = len(noise)\n        audio_length = len(waveform)\n\n        if audio_length >= noise_length:\n            shortage = audio_length - noise_length\n            noise = np.pad(noise, (0, shortage), 'wrap')\n        else:\n            start = np.random.randint(0, (noise_length-audio_length))\n            noise = noise[start:start+audio_length]\n\n        noise_dB = compute_dB(noise)\n        noise = np.sqrt(10 ** ((clean_dB - noise_dB - snr) / 10)) * noise\n        waveform = (waveform + noise)\n        return waveform\n\n    def reverberate(self, waveform):\n        \"\"\"\n        Args:\n            x (numpy.array): Input length (#length).\n        Returns:\n            numpy.array: Output waveform array (#length).\n        \"\"\"\n        audio_length = len(waveform)\n        idx = np.random.randint(0, len(self.rir_paths))\n\n        path = self.rir_paths[idx]\n        rir, sample_rate = soundfile.read(path)\n        rir = rir/np.sqrt(np.sum(rir**2))\n\n        waveform = signal.convolve(waveform, rir, mode='full')\n        return waveform[:audio_length]\nimport collections\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport librosa\nfrom scipy import signal\nfrom scipy.io import wavfile\nfrom sklearn.utils import shuffle\nfrom torch.utils.data import DataLoader, Dataset\n\ndef load_audio(filename, num_frames):\n    sample_rate, waveform = wavfile.read(filename)\n    audio_length = waveform.shape[0]\n    second = int(num_frames/100)\n\n    if second <= 0:\n        return waveform.astype(np.float64).copy()\n\n    length = np.int64(sample_rate * second)\n\n    if audio_length <= length:\n        shortage = length - audio_length\n        waveform = np.pad(waveform, (0, shortage), 'wrap')\n        waveform = waveform.astype(np.float64)\n    else:\n        start = np.int64(random.random()*(audio_length-length))\n        waveform =  waveform[start:start+length].astype(np.float64)\n    return waveform.copy()\n\nclass Train_Dataset(Dataset):\n    def __init__(self, train_csv_path, num_frames, pairs=False, **kwargs):\n        self.pairs = pairs\n        df = pd.read_csv(train_csv_path)\n        self.labels = df[\"utt_spk_int_labels\"].values\n        self.paths = df[\"utt_paths\"].values\n        self.labels, self.paths = shuffle(self.labels, self.paths)\n        self.num_frames = num_frames\n        self.wav_aug = WavAugment()\n\n        print(\"Train Dataset load {} speakers\".format(len(set(self.labels))))\n        print(\"Train Dataset load {} utterance\".format(len(self.labels)))\n\n    def __getitem__(self, index):\n        waveform_1 = load_audio(self.paths[index], self.num_frames)\n\n        waveform_1 = self.wav_aug.augment(waveform_1)\n        if self.pairs == False:\n            return torch.FloatTensor(waveform_1), self.labels[index]\n\n        else:\n            waveform_2 = load_audio(self.paths[index], self.num_frames)\n            waveform_2 = self.wav_aug.augment(waveform_2)\n            return torch.FloatTensor(waveform_1), torch.FloatTensor(waveform_2), self.labels[index]\n\n    def __len__(self):\n        return len(self.paths)\nclass Evaluation_Dataset(Dataset):\n    def __init__(self, paths, second=-1, **kwargs):\n        self.paths = paths\n        self.second = second\n        print(\"load {} utterance\".format(len(self.paths)))\n\n    def __getitem__(self, index):\n        waveform = load_audio(self.paths[index], self.second)\n        return torch.FloatTensor(waveform), self.paths[index]\n\n    def __len__(self):\n        return len(self.paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:14:51.117499Z","iopub.execute_input":"2025-05-31T03:14:51.117811Z","iopub.status.idle":"2025-05-31T03:14:51.139467Z","shell.execute_reply.started":"2025-05-31T03:14:51.117787Z","shell.execute_reply":"2025-05-31T03:14:51.138790Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Training Object","metadata":{}},{"cell_type":"code","source":"'''\nThis is the main code of the ECAPATDNN project, to define the parameters and build the construction\n'''\n\nimport argparse, glob, os, torch, warnings, time\n\nparser = argparse.ArgumentParser(description = \"ECAPA_trainer\")\n## Training Settings\nparser.add_argument('--num_frames', type=int,   default=400,     help='Duration of the input segments, eg: 200 for 2 second')\nparser.add_argument('--max_epoch',  type=int,   default=40,      help='Maximum number of epochs')\nparser.add_argument('--batch_size', type=int,   default=64,     help='Batch size')\nparser.add_argument('--n_cpu',      type=int,   default=4,       help='Number of loader threads')\nparser.add_argument('--test_step',  type=int,   default=1,       help='Test and save every [test_step] epochs')\nparser.add_argument('--lr',         type=float, default=0.001,   help='Learning rate')\nparser.add_argument(\"--second\", type=int, default=4)\nparser.add_argument(\"--lr_decay\",   type=float, default=0.9,    help='Learning rate decay every [test_step] epochs')\n\n## Training and evaluation path/lists, save path\nparser.add_argument('--train_list', type=str,   default=\"/kaggle/input/vietnam-celeb-dataset/vietnam-celeb-t.txt\",     help='The path of the training list, https://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/train_list.txt')\nparser.add_argument('--train_path', type=str,   default=\"/kaggle/input/vietnam-celeb-dataset/full-dataset/data\",                    help='The path of the training data, eg:\"/data08/VoxCeleb2/train/wav\" in my case')\nparser.add_argument('--train_csv_path', type=str, default=\"/kaggle/input/csv-files/processed_vietnam_celeb_dataset.csv\")\nparser.add_argument('--eval_list',  type=str,   default=\"/kaggle/input/vietnam-celeb-dataset/full-dataset/vietnam-celeb-h.txt\",              help='The path of the evaluation list, veri_test2.txt comes from https://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test2.txt')\nparser.add_argument('--eval_path',  type=str,   default=\"/kaggle/input/vietnam-celeb-dataset/full-dataset/data\",                    help='The path of the evaluation data, eg:\"/data08/VoxCeleb1/test/wav\" in my case')\nparser.add_argument(\"--trial_path\", type=str, default=\"/kaggle/working/vi_speech.csv\")\nparser.add_argument('--musan_path', type=str,   default=\"/kaggle/input/musan-dataset\",                    help='The path to the MUSAN set, eg:\"/data08/Others/musan_split\" in my case')\nparser.add_argument('--rir_path',   type=str,   default=\"/kaggle/input/rir-dataset/sim_rir_16k/simulated_rirs_16k\",     help='The path to the RIR set, eg:\"/data08/Others/RIRS_NOISES/simulated_rirs\" in my case');\nparser.add_argument('--save_path',  type=str,   default=\"exps/exp1\",                                     help='Path to save the score.txt and models')\nparser.add_argument('--initial_model',  type=str,   default=\"\",                                          help='Path of the initial_model')\nparser.add_argument('--aug', action='store_true')\n\n## Model and Loss settings\nparser.add_argument('--C',       type=int,   default=512,   help='Channel size for the speaker encoder')\nparser.add_argument('--m',       type=float, default=0.2,    help='Loss margin in AAM softmax')\nparser.add_argument('--s',       type=float, default=30,     help='Loss scale in AAM softmax')\nparser.add_argument('--n_class', type=int,   default=880,   help='Number of speakers')\n\n## Command\nparser.add_argument('--eval',    dest='eval', action='store_true', help='Only do evaluation')\n\n## Initialization\nwarnings.simplefilter(\"ignore\")\ntorch.multiprocessing.set_sharing_strategy('file_system')\nargs = parser.parse_args([])\nargs = init_args(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:11:15.568194Z","iopub.execute_input":"2025-05-31T03:11:15.568885Z","iopub.status.idle":"2025-05-31T03:11:15.581319Z","shell.execute_reply.started":"2025-05-31T03:11:15.568862Z","shell.execute_reply":"2025-05-31T03:11:15.580336Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"## Define the data loader\ntrain_dataset = Train_Dataset(**vars(args))\ntrainLoader = torch.utils.data.DataLoader(train_dataset, batch_size = args.batch_size, shuffle = True, num_workers = args.n_cpu, drop_last = True, pin_memory = True)\n\n## Search for the exist models\nmodelfiles = glob.glob('%s/model_0*.model'%args.model_save_path)\nmodelfiles.sort()\n\n## Only do evaluation, the initial_model is necessary\nif args.eval == True:\n\ts = ECAPAModel(**vars(args))\n\tprint(\"Model %s loaded from previous state!\"%args.initial_model)\n\ts.load_parameters(args.initial_model)\n\tEER, minDCF = s.eval_network(eval_list = args.eval_list, eval_path = args.eval_path)\n\tprint(\"EER %2.2f%%, minDCF %.4f%%\"%(EER, minDCF))\n\tquit()\n\n## If initial_model is exist, system will train from the initial_model\nif args.initial_model != \"\":\n\tprint(\"Model %s loaded from previous state!\"%args.initial_model)\n\ts = ECAPAModel(**vars(args))\n\ts.load_parameters(args.initial_model)\n\tepoch = 1\n\n## Otherwise, system will try to start from the saved model&epoch\nelif len(modelfiles) >= 1:\n\tprint(\"Model %s loaded from previous state!\"%modelfiles[-1])\n\tepoch = int(os.path.splitext(os.path.basename(modelfiles[-1]))[0][6:]) + 1\n\ts = ECAPAModel(**vars(args))\n\ts.load_parameters(modelfiles[-1])\n## Otherwise, system will train from scratch\nelse:\n\tepoch = 1\n\ts = ECAPAModel(**vars(args))\n\nEERs = []\nMinDCF = []\nscore_file = open(args.score_save_path, \"a+\")\n\nprint(f'Large Margin Finetuning with:\\nLength = {args.num_frames / 100}\\nLearning Rate = {args.lr}\\nMargin = {args.m}\\nEpochs = {args.max_epoch}')\nwhile(1):\n\t## Training for one epoch\n\tloss, lr, acc = s.train_network(epoch = epoch, loader = trainLoader)\n    \n\t## Evaluation every [test_step] epochs\n\tif epoch % args.test_step == 0:\n\t\ts.save_parameters(args.model_save_path + \"/model_%04d.model\"%epoch)\n\t\teer, mindcf = s.eval_network(eval_list = args.eval_list, eval_path = args.eval_path)\n\t\tEERs.append(eer)\n\t\tMinDCF.append(mindcf)\n\t\tprint(time.strftime(\"%Y-%m-%d %H:%M:%S\"), \"%d epoch, MinDCF %2.2f%%, EER %2.2f%%, bestEER %2.2f%%\"%(epoch, mindcf, EERs[-1], min(EERs)))\n\t\tscore_file.write(\"%d epoch, LR %f, LOSS %f, ACC %2.2f%%, MinDCF %2.2f%%, EER %2.2f%%, bestEER %2.2f%%\\n\"%(epoch, lr, loss, acc, mindcf, EERs[-1], min(EERs)))\n\t\tscore_file.flush()\n\n\tif (epoch % 1 == 0) or args.max_epoch == epoch:    \n\t\tvispeech_eer, _ = s.eval_network(eval_list = \"/kaggle/working/eval_pairs.txt\", eval_path = \"/kaggle/input/vispeech/noisy_testset\")\n\t\tprint(f\"EER on ViSpeech : {vispeech_eer:.2f}%\")      \n\tif (epoch % 3 == 0) or args.max_epoch == epoch:    \n\t\tvox_eer, _ = s.eval_network(eval_list = \"/kaggle/input/voxvietnam/test_list_gt.csv\", eval_path = \"/kaggle/input/voxvietnam/wav\")\n\t\tprint(f\"EER on Vox : {vox_eer:.2f}%\")      \n\tif epoch >= args.max_epoch:\n\t\tbreak\n\tepoch += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T03:14:59.940840Z","iopub.execute_input":"2025-05-31T03:14:59.941121Z"}},"outputs":[{"name":"stdout","text":"Train Dataset load 880 speakers\nTrain Dataset load 59417 utterance\n160 640 4\n1 640 512\n2 512 512\n3 512 512\n05-31 03:15:00 Model para number = 6.98\nLarge Margin Finetuning with:\nLength = 4.0\nLearning Rate = 0.001\nMargin = 0.2\nEpochs = 40\n","output_type":"stream"},{"name":"stderr","text":"05-31 03:19:26 [ 1] Lr: 0.001000, Training: 68.21%,  Loss: 9.74188, ACC: 5.84%  \r","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model_0 = ECAPAModel(**vars(args))\nmodel_0.load_parameters(\"/kaggle/working/exps/exp1/model/model_0020.model\")\nvox_eer, _ = model_0.eval_network(\"/kaggle/input/voxvietnam/test_list_gt.csv\", \"/kaggle/input/voxvietnam/wav\")\nprint(vox_eer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T10:37:42.120204Z","iopub.execute_input":"2025-05-25T10:37:42.120778Z","iopub.status.idle":"2025-05-25T10:43:55.563183Z","shell.execute_reply.started":"2025-05-25T10:37:42.120756Z","shell.execute_reply":"2025-05-25T10:43:55.562358Z"}},"outputs":[{"name":"stdout","text":"05-25 10:37:42 Model para number = 6.65\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17786/17786 [05:08<00:00, 57.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"5.168478990468519\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"print(train_dataset[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:38:48.000483Z","iopub.execute_input":"2025-05-25T07:38:48.000820Z","iopub.status.idle":"2025-05-25T07:38:48.028381Z","shell.execute_reply.started":"2025-05-25T07:38:48.000800Z","shell.execute_reply":"2025-05-25T07:38:48.027509Z"}},"outputs":[{"name":"stdout","text":"(tensor([ 0.3769,  0.3261,  0.2673,  ..., -0.4006, -0.4245, -0.4364]), tensor([-0.0462, -0.0335, -0.0223,  ..., -0.2159, -0.2824, -0.3720]), 261)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"with open(\"/kaggle/input/voxvietnam/test_list_gt.csv\", \"r\") as f:\n    path = f.readlines()[0].strip('\"')[:-2].split()[2]\n    file_path = os.path.join(\"/kaggle/input/voxvietnam/wav\", path)\n    print(os.path.exists(file_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T02:10:22.354392Z","iopub.execute_input":"2025-05-20T02:10:22.355179Z","iopub.status.idle":"2025-05-20T02:10:22.362640Z","shell.execute_reply.started":"2025-05-20T02:10:22.355153Z","shell.execute_reply":"2025-05-20T02:10:22.362017Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## AS Norm","metadata":{}},{"cell_type":"markdown","source":"### Utils","metadata":{}},{"cell_type":"code","source":"def read_scp(scp_file):\n    \"\"\"read scp file (also support PIPE format)\n\n    Args:\n        scp_file (str): path to the scp file\n\n    Returns:\n        list: key_value_list\n    \"\"\"\n    key_value_list = []\n    with open(scp_file, \"r\", encoding='utf8') as fin:\n        for line in fin:\n            tokens = line.strip().split()\n            key = tokens[0]\n            value = \" \".join(tokens[1:])\n            key_value_list.append((key, value))\n    return key_value_list\n\n\ndef read_lists(list_file):\n    \"\"\"read list file with only 1 column\n\n    Args:\n        list_file (str): path to the list file\n\n    Returns:\n        list: lists\n    \"\"\"\n    lists = []\n    with open(list_file, 'r', encoding='utf8') as fin:\n        for line in fin:\n            lists.append(line.strip())\n    return lists\n\n\ndef read_table(table_file):\n    \"\"\"read table file with any columns\n\n    Args:\n        table_file (str): path to the table file\n\n    Returns:\n        list: table_list\n    \"\"\"\n    table_list = []\n    with open(table_file, 'r', encoding='utf8') as fin:\n        for line in fin:\n            tokens = line.strip().split()\n            table_list.append(tokens)\n    return table_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T14:29:43.483367Z","iopub.execute_input":"2025-05-22T14:29:43.483907Z","iopub.status.idle":"2025-05-22T14:29:43.489883Z","shell.execute_reply.started":"2025-05-22T14:29:43.483883Z","shell.execute_reply":"2025-05-22T14:29:43.489179Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"### Score Norm","metadata":{}},{"cell_type":"code","source":"pip install fire","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T14:29:46.877958Z","iopub.execute_input":"2025-05-22T14:29:46.878508Z","iopub.status.idle":"2025-05-22T14:29:51.896201Z","shell.execute_reply.started":"2025-05-22T14:29:46.878485Z","shell.execute_reply":"2025-05-22T14:29:51.895389Z"}},"outputs":[{"name":"stdout","text":"Collecting fire\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire) (2.5.0)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=b5a79a11df056b20eda141374a20d0372405724533b696c6c6a57a46ac1fce07\n  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\nSuccessfully built fire\nInstalling collected packages: fire\nSuccessfully installed fire-0.7.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import logging\nimport os\n\nimport fire\nimport kaldiio\nimport numpy as np\nfrom tqdm import tqdm\nfrom scipy.special import expit\n\n\n\ndef get_mean_std(emb, cohort, top_n):\n    emb, cohort = emb.squeeze(), cohort.squeeze()\n    emb = emb / np.sqrt(np.sum(emb**2, axis=1, keepdims=True))\n    cohort = cohort / np.sqrt(np.sum(cohort**2, axis=1, keepdims=True))\n    emb_cohort_score = np.matmul(emb, cohort.T)\n    emb_cohort_score = np.sort(emb_cohort_score, axis=1)[:, ::-1]\n    emb_cohort_score_topn = emb_cohort_score[:, :top_n]\n\n    emb_mean = np.mean(emb_cohort_score_topn, axis=1)\n    emb_std = np.std(emb_cohort_score_topn, axis=1)\n\n    return emb_mean, emb_std\n\n\ndef split_embedding(utt_list, emb_scp, mean_vec):\n    embs = []\n    utt2idx = {}\n    utt2emb = {}\n    for utt, emb in kaldiio.load_scp_sequential(emb_scp):\n        emb = emb.squeeze()\n        emb = emb - mean_vec\n        utt2emb[utt] = emb\n\n    for utt in utt_list:\n        embs.append(utt2emb[utt])\n        utt2idx[utt] = len(embs) - 1\n\n    return np.array(embs), utt2idx\n\n\ndef as_norm(score_norm_method,\n         top_n,\n         trial_score_file,\n         score_norm_file,\n         cohort_emb_scp,\n         eval_emb_scp,\n         mean_vec_path=None):\n    logging.basicConfig(level=logging.INFO,\n                        format='%(asctime)s %(levelname)s %(message)s')\n    # get embedding\n    if not mean_vec_path:\n        print(\"Do not do mean normalization for evaluation embeddings.\")\n        mean_vec = 0.0\n    else:\n        assert os.path.exists(\n            mean_vec_path), \"mean_vec file ({}) does not exist !!!\".format(\n                mean_vec_path)\n        mean_vec = np.load(mean_vec_path)\n\n    # get embedding\n    logging.info('get embedding ...')\n\n    enroll_list, test_list, _ = zip(*read_table(trial_score_file))\n    enroll_list = sorted(list(set(enroll_list)))  # remove overlap and sort\n    test_list = sorted(list(set(test_list)))\n    enroll_emb, enroll_utt2idx = split_embedding(enroll_list, eval_emb_scp,\n                                                 mean_vec)\n    test_emb, test_utt2idx = split_embedding(test_list, eval_emb_scp, mean_vec)\n\n    cohort_list, _ = zip(*read_table(cohort_emb_scp))\n    cohort_emb, _ = split_embedding(cohort_list, cohort_emb_scp, mean_vec)\n\n    logging.info(\"computing normed score ...\")\n    if score_norm_method == \"asnorm\":\n        top_n = top_n\n    elif score_norm_method == \"snorm\":\n        top_n = cohort_emb.shape[0]\n    else:\n        raise ValueError(score_norm_method)\n    enroll_mean, enroll_std = get_mean_std(enroll_emb, cohort_emb, top_n)\n    test_mean, test_std = get_mean_std(test_emb, cohort_emb, top_n)\n\n    # score norm\n    with open(trial_score_file, 'r', encoding='utf-8') as fin:\n        with open(score_norm_file, 'w', encoding='utf-8') as fout:\n            lines = fin.readlines()\n            for line in tqdm(lines):\n                line = line.strip().split()\n                enroll_idx = enroll_utt2idx[line[0]]\n                test_idx = test_utt2idx[line[1]]\n                score = float(line[2])\n                normed_score = 0.5 * (\n                    (score - enroll_mean[enroll_idx]) / enroll_std[enroll_idx]\n                    + (score - test_mean[test_idx]) / test_std[test_idx])\n                normed_score = expit(normed_score)\n                # compute mag mean for score calibration\n                enroll_mag = np.linalg.norm(enroll_emb[enroll_idx])\n                test_mag = np.linalg.norm(test_emb[test_idx])\n                \"\"\"\n                fout.write(\n                    '{} {} {:.5f} {:.4f} {:.4f} {:.4f} {:.4f}\\n'.format(\n                        line[0], line[1], normed_score, enroll_mag,\n                        test_mag, enroll_mean[enroll_idx],\n                        test_mean[test_idx]))\n                \"\"\"\n                fout.write(f\"{normed_score}\\n\")\ntrial_score_file = \"/kaggle/working/trial_test_list.txt\"\nscore_norm_file = \"predictions.txt\"\ncohort_emb_scp = \"/kaggle/working/cohort_emb.scp\"\neval_emb_scp = \"/kaggle/working/test_emb.scp\"\nas_norm(\"asnorm\", 250, trial_score_file, score_norm_file, cohort_emb_scp, eval_emb_scp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:26:48.950470Z","iopub.execute_input":"2025-05-22T15:26:48.951127Z","iopub.status.idle":"2025-05-22T15:26:52.573764Z","shell.execute_reply.started":"2025-05-22T15:26:48.951102Z","shell.execute_reply":"2025-05-22T15:26:52.573017Z"}},"outputs":[{"name":"stdout","text":"Do not do mean normalization for evaluation embeddings.\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 15971/15971 [00:00<00:00, 69918.35it/s]\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"with open(\"predictions.txt\", \"r\") as score_file:\n    scores = []\n    for line in score_file.readlines():\n        score = float(line.split()[2])\n        scores.append(score)\nwith open(\"/kaggle/input/voxvietnam/test_list_gt.csv\", \"r\") as test_file:\n    labels = []\n    for line in test_file.readlines():\n        label = line.strip('\"').split()[0]\n        labels.append(int(label))\neer = tuneThresholdfromScore(scores, labels, [1, 0.1])[1]\nprint(f\"EER after AS Norm: {eer}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"predictions.txt\", \"r\") as score_file:\n    scores = []\n    for score in score_file.readlines():\n        score = float(score)\n        scores.append(score)\nprint(len(scores))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T10:54:33.202431Z","iopub.execute_input":"2025-05-21T10:54:33.202732Z","iopub.status.idle":"2025-05-21T10:54:33.211173Z","shell.execute_reply.started":"2025-05-21T10:54:33.202712Z","shell.execute_reply":"2025-05-21T10:54:33.210418Z"}},"outputs":[{"name":"stdout","text":"9895\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Cohort Sets","metadata":{}},{"cell_type":"code","source":"import tqdm\nmodel_0 = ECAPAModel(**vars(args))\nmodel_0.load_parameters(\"/kaggle/input/ecapa_sv/pytorch/default/7/model_finetune_ph2_ckp6.model\")\ndef get_raw_scores(test_list, test_path, output_file = \"/kaggle/working/trial_test_list.txt\"):\n    scores = model_0.test_network(test_list, test_path)\n    with open(test_list, \"r\") as f:\n        with open(\"trial_test_list.txt\", \"w\") as out_f:\n            for idx, line in enumerate(f.readlines()):\n                test_id, enroll_id = line.split() \n                out_f.write(f\"enroll_utt_{enroll_id} test_utt_{test_id} {scores[idx]}\\n\")\n                if (idx + 1) % 1000 == 0:\n                    print(f\"Step : {idx + 1}\")\nget_raw_scores(\"/kaggle/input/privatesvtest/prompts_sv.csv\", \"/kaggle/input/privatesvtest/audio\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:17:23.321706Z","iopub.execute_input":"2025-05-22T15:17:23.322075Z","iopub.status.idle":"2025-05-22T15:19:33.181310Z","shell.execute_reply.started":"2025-05-22T15:17:23.322054Z","shell.execute_reply":"2025-05-22T15:19:33.180622Z"}},"outputs":[{"name":"stdout","text":"05-22 15:17:23 Model para number = 6.65\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6438/6438 [02:07<00:00, 50.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Step : 1000\nStep : 2000\nStep : 3000\nStep : 4000\nStep : 5000\nStep : 6000\nStep : 7000\nStep : 8000\nStep : 9000\nStep : 10000\nStep : 11000\nStep : 12000\nStep : 13000\nStep : 14000\nStep : 15000\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"pip install kaldiio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T14:08:55.438573Z","iopub.execute_input":"2025-05-22T14:08:55.438881Z","iopub.status.idle":"2025-05-22T14:08:59.604549Z","shell.execute_reply.started":"2025-05-22T14:08:55.438860Z","shell.execute_reply":"2025-05-22T14:08:59.603493Z"}},"outputs":[{"name":"stdout","text":"Collecting kaldiio\n  Downloading kaldiio-2.18.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from kaldiio) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->kaldiio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->kaldiio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->kaldiio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->kaldiio) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->kaldiio) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->kaldiio) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->kaldiio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->kaldiio) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->kaldiio) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->kaldiio) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->kaldiio) (2024.2.0)\nDownloading kaldiio-2.18.1-py3-none-any.whl (29 kB)\nInstalling collected packages: kaldiio\nSuccessfully installed kaldiio-2.18.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import kaldiio\ndef get_test_emb(test_list, test_path, ark_file = \"test_emb.ark\", scp_file = \"test_emb.scp\"):\n    write_specifier = f\"ark,scp:{ark_file},{scp_file}\"\n    with open(test_list, \"r\") as test_f:\n        lines = test_f.readlines()\n    with torch.inference_mode(), kaldiio.WriteHelper(write_specifier) as writer:\n        idx = 0\n        for line in lines:\n            idx += 1\n            test_id, enroll_id = line.split()\n            test_file = os.path.join(test_path, test_id)\n            enroll_file = os.path.join(test_path, enroll_id)\n            \n            test_embedding = model_0.forward(test_file)\n            enroll_embedding = model_0.forward(enroll_file)\n\n            test_key = f\"test_utt_{test_id}\"\n            enroll_key = f\"enroll_utt_{enroll_id}\"\n            writer[test_key] = test_embedding.cpu().numpy()\n            writer[enroll_key] = enroll_embedding.cpu().numpy()\n            if (idx + 1) % 1000 == 0:\n                print(f\"Step : {idx + 1}\")\nget_test_emb(\"/kaggle/input/privatesvtest/prompts_sv.csv\", \"/kaggle/input/privatesvtest/audio\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:19:58.000477Z","iopub.execute_input":"2025-05-22T15:19:58.000924Z","iopub.status.idle":"2025-05-22T15:25:32.270392Z","shell.execute_reply.started":"2025-05-22T15:19:58.000897Z","shell.execute_reply":"2025-05-22T15:25:32.269629Z"}},"outputs":[{"name":"stdout","text":"Step : 1000\nStep : 2000\nStep : 3000\nStep : 4000\nStep : 5000\nStep : 6000\nStep : 7000\nStep : 8000\nStep : 9000\nStep : 10000\nStep : 11000\nStep : 12000\nStep : 13000\nStep : 14000\nStep : 15000\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"import random\nwith open(\"/kaggle/input/vietnam-celeb-dataset/vietnam-celeb-t.txt\", \"r\") as f:\n    lines = f.readlines()\n    sample_lines = random.sample(lines, 30000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:01:22.562217Z","iopub.execute_input":"2025-05-22T15:01:22.562776Z","iopub.status.idle":"2025-05-22T15:01:22.594991Z","shell.execute_reply.started":"2025-05-22T15:01:22.562750Z","shell.execute_reply":"2025-05-22T15:01:22.594258Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"def get_cohort_emb(cohort_loader, ark_file = \"cohort_emb.ark\", scp_file = \"cohort_emb.scp\"):\n    write_specifier = f\"ark,scp:{ark_file},{scp_file}\"\n    with torch.inference_mode(), kaldiio.WriteHelper(write_specifier) as writer:\n\t\tfor num, (data, labels) in enumerate(cohort_loader, start=1):\n\t\t\t# Move data to device efficiently\n\t\t\t# Assuming data is already a tensor from the optimized DataLoader\n\t\t\tif isinstance(data, torch.Tensor):\n\t\t\t\tdata = data.to(self.device, non_blocking=True)\n\t\t\telse:\n\t\t\t\tdata = torch.FloatTensor(data).to(self.device, non_blocking=True)\n\t\t\tif isinstance(labels, torch.Tensor):\n\t\t\t\tlabels = labels.to(self.device, non_blocking=True)\n\t\t\telse:\n\t\t\t\tlabels = torch.LongTensor(labels).to(self.device, non_blocking=True)\n\n            embedding = self.speaker_encoder.forward(data, aug=True)\n\n            embedding = embedding.cpu().numpy()\n\n            key = f\"utt_{labels}_{num}\"\n            writer[key] = embedding\n            if (num+ 1) % 1000 == 0:\n                print(f\"Step : {num + 1}\")\n                \nget_cohort_emb(\"/kaggle/input/csv-files/processed_vietnam_celeb_dataset.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T15:01:24.469969Z","iopub.execute_input":"2025-05-22T15:01:24.470564Z","iopub.status.idle":"2025-05-22T15:08:05.315474Z","shell.execute_reply.started":"2025-05-22T15:01:24.470543Z","shell.execute_reply":"2025-05-22T15:08:05.314850Z"}},"outputs":[{"name":"stdout","text":"Step : 1000\nStep : 3000\nStep : 4000\nStep : 5000\nStep : 6000\nStep : 7000\nStep : 8000\nStep : 9000\nStep : 10000\nStep : 12000\nStep : 13000\nStep : 14000\nStep : 17000\nStep : 18000\nStep : 19000\nStep : 20000\nStep : 21000\nStep : 22000\nStep : 24000\nStep : 25000\nStep : 26000\nStep : 27000\nStep : 28000\nStep : 30000\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"with open(\"cohort_emb.scp\",  \"r\") as f:\n    print(len(f.readlines()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T14:43:42.771120Z","iopub.execute_input":"2025-05-22T14:43:42.771722Z","iopub.status.idle":"2025-05-22T14:43:42.777079Z","shell.execute_reply.started":"2025-05-22T14:43:42.771698Z","shell.execute_reply":"2025-05-22T14:43:42.776497Z"}},"outputs":[{"name":"stdout","text":"6438\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"## Calibration","metadata":{}},{"cell_type":"markdown","source":"### Wave to Duration","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\n# encoding: utf-8\nimport os\nimport sys\n\nimport torchaudio\n\ndef wav2dur(scp, dur_scp, file_path):\n    torchaudio.set_audio_backend(\"sox_io\")\n    \n    with open(scp, 'r') as f, open(dur_scp, 'w') as fout:\n        cnt = 0\n        total_duration = 0\n        for l in sample_lines:\n            items = l.strip().split()\n            wav_id = items[0]\n            fname = items[1]\n            cnt += 1\n            path = os.path.join(file_path, wav_id, fname)\n            if not os.path.exists(path):\n                continue\n            waveform, rate = torchaudio.load(path)\n            frames = len(waveform[0])\n            duration = frames / float(rate)\n            total_duration += duration\n            fout.write('{} {}\\n'.format(wav_id, duration))\n        print('process {} utts'.format(cnt))\n        print('total {} s'.format(total_duration))\nwav2dur(\"/kaggle/input/vietnam-celeb-dataset/vietnam-celeb-t.txt\", \"vn_cl_dur.scp\", \"/kaggle/input/vietnam-celeb-dataset/full-dataset/data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:04:51.470084Z","iopub.execute_input":"2025-05-22T10:04:51.470616Z","iopub.status.idle":"2025-05-22T10:08:17.504896Z","shell.execute_reply.started":"2025-05-22T10:04:51.470594Z","shell.execute_reply":"2025-05-22T10:08:17.504073Z"}},"outputs":[{"name":"stdout","text":"process 30000 utts\ntotal 158434.16118750002 s\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"with open(\"/kaggle/working/vn_cl_dur\", \"r\") as dur_f:\n    print(len(dur_f.readlines()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:08:22.650729Z","iopub.execute_input":"2025-05-22T10:08:22.651195Z","iopub.status.idle":"2025-05-22T10:08:22.657856Z","shell.execute_reply.started":"2025-05-22T10:08:22.651173Z","shell.execute_reply":"2025-05-22T10:08:22.657244Z"}},"outputs":[{"name":"stdout","text":"21926\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"### Generate Calibration Trials","metadata":{}},{"cell_type":"code","source":"pip install fire","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:08:30.110081Z","iopub.execute_input":"2025-05-22T10:08:30.110640Z","iopub.status.idle":"2025-05-22T10:08:32.997969Z","shell.execute_reply.started":"2025-05-22T10:08:30.110619Z","shell.execute_reply":"2025-05-22T10:08:32.996998Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (0.7.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire) (2.5.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import fire\nimport logging\nimport random\nfrom tqdm import tqdm\n\n\ndef generate_calibration_trials(utt2dur, trial_path, each_trial_num=10000):\n    logging.basicConfig(level=logging.INFO,\n                        format='%(asctime)s %(levelname)s %(message)s')\n    logging.info('Generate calibration trial ...')\n    short_spk2utt = {}\n    long_spk2utt = {}\n\n    with open(utt2dur, 'r') as f:\n        for line in f.readlines():\n            utt, dur = line.strip().split()\n            dur = float(dur)\n            spk = utt.split('/')[0]\n\n            if 2 < dur < 6:\n                if spk not in short_spk2utt:\n                    short_spk2utt[spk] = []\n                short_spk2utt[spk].append(utt)\n\n            if dur > 6:\n                if spk not in long_spk2utt:\n                    long_spk2utt[spk] = []\n                long_spk2utt[spk].append(utt)\n\n    long_spks = list(long_spk2utt.keys())\n    short_spks = list(short_spk2utt.keys())\n\n    for spk in long_spks:\n        if spk not in short_spks:\n            long_spk2utt.pop(spk, None)\n    long_spks = list(long_spk2utt.keys())\n    for spk in short_spks:\n        if spk not in long_spks:\n            short_spk2utt.pop(spk, None)\n    short_spks = list(short_spk2utt.keys())\n\n    with open(trial_path, 'w') as f:\n        for _ in tqdm(range(each_trial_num // 2)):\n            enroll_spk = random.choice(short_spks)\n            spk_index = short_spks.index(enroll_spk)\n            nontarget_spk = random.choice(short_spks[:spk_index] +\n                                          short_spks[spk_index + 1:])\n\n            # short2short\n            enroll_utt, test_utt = random.choices(short_spk2utt[enroll_spk],\n                                                  k=2)\n            f.write(\"{} {} {}\\n\".format(enroll_utt, test_utt, 'target'))\n            test_utt = random.choice(short_spk2utt[nontarget_spk])\n            f.write(\"{} {} {}\\n\".format(enroll_utt, test_utt, 'nontarget'))\n\n            # short2long\n            enroll_utt = random.choice(short_spk2utt[enroll_spk])\n            test_utt = random.choice(long_spk2utt[enroll_spk])\n            f.write(\"{} {} {}\\n\".format(enroll_utt, test_utt, 'target'))\n            test_utt = random.choice(long_spk2utt[nontarget_spk])\n            f.write(\"{} {} {}\\n\".format(enroll_utt, test_utt, 'nontarget'))\n\n            # long2long\n            enroll_utt, test_utt = random.choices(long_spk2utt[enroll_spk],\n                                                  k=2)\n            f.write(\"{} {} {}\\n\".format(enroll_utt, test_utt, 'target'))\n            test_utt = random.choice(long_spk2utt[nontarget_spk])\n            f.write(\"{} {} {}\\n\".format(enroll_utt, test_utt, 'nontarget'))\ngenerate_calibration_trials(\"/kaggle/working/vn_cl_dur.scp\", \"vn_cl_trials\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:10:43.232925Z","iopub.execute_input":"2025-05-22T10:10:43.233642Z","iopub.status.idle":"2025-05-22T10:10:43.355755Z","shell.execute_reply.started":"2025-05-22T10:10:43.233617Z","shell.execute_reply":"2025-05-22T10:10:43.355033Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 5000/5000 [00:00<00:00, 59451.51it/s]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Copyright (c) 2022 Chengdong Liang (liangchengdong@mail.nwpu.edu.cn)\n#               2024 Zhengyang Chen (chenzhengyang117@gmail.com)\n#               2024 Bing Han (hanbing97@sjtu.edu.cn)\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport fire\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\n\ndef gather_calibration_factors(wav_dur_scp, max_dur, score_norm_file,\n                               calibration_factor_file, drop_duration=False):\n    if not drop_duration:\n        wav_idx, dur_list = zip(*read_table(wav_dur_scp))\n        wavidx2dur = {\n            idx: min(float(dur), max_dur)\n            for idx, dur in zip(wav_idx, dur_list)\n        }\n\n    def reorder_values(value_1, value_2):\n        max_value = max(value_1, value_2)\n        min_value = min(value_1, value_2)\n        return \"{:.4f} {:.4f} {:.4f} {:.4f}\".format(min_value, max_value,\n                                                    max_value - min_value,\n                                                    max_value / min_value)\n\n    # read factor from asnorm results\n    assert os.path.exists(\n        score_norm_file), \"score norm file ({}) does not exist !!!\".format(\n            score_norm_file)\n\n    with open(score_norm_file, 'r', encoding='utf-8') as fin:\n        with open(calibration_factor_file, 'w', encoding='utf-8') as fout:\n            lines = fin.readlines()\n            for line in tqdm(lines):\n                line = line.strip().split()\n                idx1, idx2 = line[0], line[1]\n                if drop_duration:\n                    dur_str = \"\"\n                else:\n                    dur_str = reorder_values(wavidx2dur[idx1], wavidx2dur[idx2])\n                mag_str = reorder_values(float(line[3]), float(line[4]))\n                cohort_mean_str = reorder_values(float(line[5]),\n                                                 float(line[6]))\n                fout.write('{} {} {} {} {} {}\\n'.format(\n                    line[0], line[1], line[2], dur_str, mag_str,\n                    cohort_mean_str))\n\n\nclass LinearModel(nn.Module):\n\n    def __init__(self, input_dim):\n        super(LinearModel, self).__init__()\n        self.linear = nn.Linear(input_dim, 1)\n        nn.init.constant_(self.linear.weight, 1.0 / input_dim)\n        nn.init.constant_(self.linear.bias, 0)\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n\ndef cllr(target_llrs, nontarget_llrs):\n    \"\"\"\n    Calculate the CLLR of the scores\n    \"\"\"\n\n    def negative_log_sigmoid(lodds):\n        \"\"\"-log(sigmoid(log_odds))\"\"\"\n        return torch.log1p(torch.exp(-lodds))\n\n    return 0.5 * (torch.mean(negative_log_sigmoid(target_llrs)) + torch.mean(\n        negative_log_sigmoid(-nontarget_llrs))) / np.log(2)\n\n\ndef train_calibration_model(calibration_factor_file, save_model_path):\n    max_epochs = 50\n    target_llrs_list = []\n    nontarget_llrs_list = []\n    with open(calibration_factor_file, 'r', encoding='utf-8') as fin:\n        lines = fin.readlines()\n        for line in lines:\n            line = line.strip().split()\n            if line[2] == \"tgt\" or line[2] == \"target\":\n                target_llrs_list.append([float(v) for v in line[3:]])\n            else:\n                nontarget_llrs_list.append([float(v) for v in line[3:]])\n\n    # build training set\n    target_llrs = torch.tensor(target_llrs_list, dtype=torch.float64)\n    nontarget_llrs = torch.tensor(nontarget_llrs_list, dtype=torch.float64)\n    start_cllr = cllr(target_llrs, nontarget_llrs)\n\n    # create model\n    model = LinearModel(target_llrs.shape[-1])\n    model.double()\n    criterion = cllr\n\n    # build optimizer\n    optimizer = optim.LBFGS(model.parameters(), lr=0.01)\n\n    best_loss = 1000000.0\n    for i in range(max_epochs):\n\n        def closure():\n            optimizer.zero_grad()\n            new_nontarget_llrs = model(nontarget_llrs)\n            new_target_llrs = model(target_llrs)\n            loss = criterion(new_target_llrs, new_nontarget_llrs)\n            loss.backward()\n            return loss\n\n        loss = optimizer.step(closure)\n        if (best_loss - loss < 1e-4):\n            break\n        else:\n            if loss < best_loss:\n                best_loss = loss\n\n    torch.save(model.state_dict(), save_model_path)\n\n\ndef infer_calibration(calibration_factor_file, save_model_path,\n                      calibration_score_file):\n    llrs_list = []\n    with open(calibration_factor_file, 'r', encoding='utf-8') as fin:\n        lines = fin.readlines()\n        for line in lines:\n            line = line.strip().split()\n            llrs_list.append([float(v) for v in line[3:]])\n\n    llrs = torch.tensor(llrs_list, dtype=torch.float64)\n\n    model = LinearModel(llrs.shape[-1])\n    model.load_state_dict(torch.load(save_model_path))\n    model.eval()\n    model.double()\n    outputs = model(llrs)\n\n    with open(calibration_score_file, \"w\", encoding='utf-8') as fout:\n        for i, s in enumerate(lines):\n            line = lines[i].strip().split()\n            score = outputs[i].item()\n            fout.write('{} {} {} {}\\n'.format(line[0], line[1], score,\n                                              line[2]))\n\ngather_calibration_factors(\"/kaggle/working/vn_cl_dur.scp\", 20, score_norm_file, \"vn_cl_factors.calibration\", False)\ntrain_calibration_model(\"/kaggle/working/vn_cl_factors.calibration\", \"calibration_model.pt\")\ninfer_calibration(\"/kaggle/working/vn_cl_factors\", \"/kaggle/working/calibration_model.pt\", \"predictions.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:11:14.113485Z","iopub.execute_input":"2025-05-22T10:11:14.114052Z","iopub.status.idle":"2025-05-22T10:11:14.165655Z","shell.execute_reply.started":"2025-05-22T10:11:14.114034Z","shell.execute_reply":"2025-05-22T10:11:14.164789Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/9895 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/395519874.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m                                               line[2]))\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m \u001b[0mgather_calibration_factors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/vn_cl_dur\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_norm_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vn_cl_factors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0mtrain_calibration_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/vn_cl_factors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"calibration_model.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0minfer_calibration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/vn_cl_factors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/kaggle/working/calibration_model.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predictions.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/395519874.py\u001b[0m in \u001b[0;36mgather_calibration_factors\u001b[0;34m(wav_dur_scp, max_dur, score_norm_file, calibration_factor_file, drop_duration)\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mdur_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                     \u001b[0mdur_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreorder_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwavidx2dur\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwavidx2dur\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mmag_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreorder_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 cohort_mean_str = reorder_values(float(line[5]),\n","\u001b[0;31mKeyError\u001b[0m: 'enroll_utt_audio00002.wav'"],"ename":"KeyError","evalue":"'enroll_utt_audio00002.wav'","output_type":"error"}],"execution_count":30},{"cell_type":"code","source":"import pandas as pd\nimport random\n\n\ndef generate_speaker_pairs(csv_file, output_file = \"eval_pairs.txt\", num_pairs=10000):\n    # Load the dataset\n    data = pd.read_csv(csv_file)\n    \n    # Group audio files by speaker ID\n    speakers = data.groupby('speaker')['audio_name'].apply(list).to_dict()\n    all_speakers = list(speakers.keys())\n    pairs = []\n    \n    # Generate positive pairs (same speaker)\n    while len(pairs) < num_pairs // 2:\n        speaker = random.choice(all_speakers)\n        if len(speakers[speaker]) < 2:\n            continue\n        file1, file2 = random.sample(speakers[speaker], 2)\n        pairs.append([1, file1, file2])\n    \n    # Generate negative pairs (different speakers)\n    while len(pairs) < num_pairs:\n        sp1, sp2 = random.sample(all_speakers, 2)\n        file1 = random.choice(speakers[sp1])\n        file2 = random.choice(speakers[sp2])\n        pairs.append([0, file1, file2])\n    \n    with open(output_file, \"w\") as fout:\n        for pair in pairs:\n            fout.write(f\"{pair[0]} {pair[1]} {pair[2]}\\n\")\ntest_path = \"/kaggle/input/vispeech/metadata/noisy_testset.csv\"\ngenerate_speaker_pairs(test_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T07:40:35.201794Z","iopub.execute_input":"2025-05-25T07:40:35.202103Z","iopub.status.idle":"2025-05-25T07:40:35.261804Z","shell.execute_reply.started":"2025-05-25T07:40:35.202077Z","shell.execute_reply":"2025-05-25T07:40:35.261212Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"markdown","source":"### ViSpeech","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport random\n\n\ndef generate_speaker_pairs(csv_file, output_file = \"test_pairs\", num_pairs=10000):\n    # Load the dataset\n    data = pd.read_csv(csv_file)\n    \n    # Group audio files by speaker ID\n    speakers = data.groupby('speaker')['audio_name'].apply(list).to_dict()\n    all_speakers = list(speakers.keys())\n    pairs = []\n    \n    # Generate positive pairs (same speaker)\n    while len(pairs) < num_pairs // 2:\n        speaker = random.choice(all_speakers)\n        if len(speakers[speaker]) < 2:\n            continue\n        file1, file2 = random.sample(speakers[speaker], 2)\n        pairs.append([1, file1, file2])\n    \n    # Generate negative pairs (different speakers)\n    while len(pairs) < num_pairs:\n        sp1, sp2 = random.sample(all_speakers, 2)\n        file1 = random.choice(speakers[sp1])\n        file2 = random.choice(speakers[sp2])\n        pairs.append([0, file1, file2])\n    \n    with open(output_file)\n    print(f\"Generated {num_pairs} pairs saved to {output_file}\")\ntest_path = \"/kaggle/input/vispeech/metadata/noisy_testset.csv\"\ngenerate_speaker_pairs(test_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T14:07:10.298660Z","iopub.execute_input":"2025-05-22T14:07:10.299519Z","iopub.status.idle":"2025-05-22T14:07:10.787635Z","shell.execute_reply.started":"2025-05-22T14:07:10.299486Z","shell.execute_reply":"2025-05-22T14:07:10.787003Z"}},"outputs":[{"name":"stdout","text":"Generated 10000 pairs saved to test_pairs\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model_0 = ECAPAModel(**vars(args))\nmodel_0.load_parameters(\"/kaggle/working/exps/exp1/model/model_0002.model\")\nmodel_0.test_network(\"/kaggle/input/privatesvtest/prompts_sv.csv\", \"/kaggle/input/privatesvtest/audio\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T08:35:36.787562Z","iopub.execute_input":"2025-05-21T08:35:36.788286Z","iopub.status.idle":"2025-05-21T08:39:39.323767Z","shell.execute_reply.started":"2025-05-21T08:35:36.788253Z","shell.execute_reply":"2025-05-21T08:39:39.322939Z"}},"outputs":[{"name":"stdout","text":"05-21 08:35:36 Model para number = 6.65\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6438/6438 [04:00<00:00, 26.82it/s]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model_0 = ECAPAModel(**vars(args))\nmodel_0.load_parameters(\"/kaggle/input/ecapa_sv/pytorch/default/4/model_20_05_ckp20.model\")\nmodel_0.eval_network(eval_list = \"/kaggle/input/voxvietnam/test_list_gt.csv\", eval_path = \"/kaggle/input/voxvietnam/wav\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T02:25:55.369375Z","iopub.execute_input":"2025-05-21T02:25:55.369657Z","iopub.status.idle":"2025-05-21T02:31:22.697456Z","shell.execute_reply.started":"2025-05-21T02:25:55.369635Z","shell.execute_reply":"2025-05-21T02:31:22.696678Z"}},"outputs":[{"name":"stdout","text":"05-21 02:25:55 Model para number = 6.65\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 17786/17786 [05:16<00:00, 56.12it/s]\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(3.5575244999328772, 0.5105657143967374)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"with open(\"predictions.txt\", \"r\") as f:\n    print(len(f.readlines()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T02:25:23.299516Z","iopub.execute_input":"2025-05-21T02:25:23.299855Z","iopub.status.idle":"2025-05-21T02:25:23.306313Z","shell.execute_reply.started":"2025-05-21T02:25:23.299832Z","shell.execute_reply":"2025-05-21T02:25:23.305561Z"}},"outputs":[{"name":"stdout","text":"15971\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from torch.utils.data import Dataset\nclass PrivateSVDataset(Dataset):\n    def __init__(self, val_path, root_dir, sr=16000):\n        \"\"\"\n        Dataset for speaker verification validation using pre-defined utterance pairs\n        \n        Args:\n            val_path: Path to validation file with pre-defined pairs\n            root_dir: Root directory containing the audio files\n            sr: Sample rate\n            duration: Max duration in seconds\n        \"\"\"\n        self.root_dir = root_dir\n        self.sr = sr\n        # Store pairs and labels\n        self.pairs = []\n        # Read validation file with pairs\n        with open(val_path, 'r') as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) == 2:\n                    utt_path1, utt_path2 = parts\n                    \n                    # Create full paths\n                    audio_path1 = os.path.join(self.root_dir, utt_path1)\n                    audio_path2 = os.path.join(self.root_dir, utt_path2)\n                    \n                    # Check if both files exist\n                    if os.path.exists(audio_path1) and os.path.exists(audio_path2):\n                        self.pairs.append((audio_path1, audio_path2))\n    \n    def __len__(self):\n        return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        audio_path1, audio_path2 = self.pairs[idx]\n        \n        # Load and process first waveform\n        waveform1 = preprocess_audio(audio_path1)\n        \n        # Load and process second waveform\n        waveform2 = preprocess_audio(audio_path2)\n        \n        return {\n            'input_values': waveform1.squeeze(),\n            'input_values2': waveform2.squeeze()\n        }\ncsv_path = \"/kaggle/input/privatesvtest/prompts_sv.csv\"\nroot_dir = \"/kaggle/input/privatesvtest/audio\"\ntest_dataset = PrivateSVDataset(csv_path, root_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T09:50:24.790681Z","iopub.execute_input":"2025-05-12T09:50:24.791473Z","iopub.status.idle":"2025-05-12T09:50:49.511705Z","shell.execute_reply.started":"2025-05-12T09:50:24.791423Z","shell.execute_reply":"2025-05-12T09:50:49.511037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from safetensors.torch import load_file\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_0 = ECAPA_TDNN().to(device)\n\nstate_dict = load_file(\"/kaggle/working/checkpoint/checkpoint-2360/model.safetensors\")\nmodel_0.load_state_dict(state_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T10:30:00.541327Z","iopub.execute_input":"2025-05-16T10:30:00.541598Z","iopub.status.idle":"2025-05-16T10:30:00.625069Z","shell.execute_reply.started":"2025-05-16T10:30:00.541575Z","shell.execute_reply":"2025-05-16T10:30:00.624521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.nn import CosineSimilarity\n\ncosine_sim = CosineSimilarity(dim = 1, eps = 1e-6)\n\nmodel_0.eval()\nwith torch.inference_mode(), open(\"predictions.txt\", \"w\") as txtfile:\n    for idx in range(len(test_dataset)):\n        data = test_dataset[idx]\n        test_data = data[\"input_values\"]\n        enroll_data = data[\"input_values2\"]\n\n        test_output = model_0(test_data.unsqueeze(0).to(device))\n        enroll_output = model_0(enroll_data.unsqueeze(0).to(device))\n\n        logit = cosine_sim(test_output, enroll_output)\n        out_logit = logit[0].detach().item()\n        txtfile.write(f\"{out_logit:.2f}\")\n        txtfile.write(\"\\n\")\n        if (idx + 1) % 1000 == 0:\n            print(f\"Step : {idx + 1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T10:34:20.092514Z","iopub.execute_input":"2025-05-16T10:34:20.092757Z","iopub.status.idle":"2025-05-16T10:42:09.857168Z","shell.execute_reply.started":"2025-05-16T10:34:20.092741Z","shell.execute_reply":"2025-05-16T10:42:09.856532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"predictions.txt\", \"r\") as f:\n    print(len(f.read().splitlines()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T10:03:22.134410Z","iopub.execute_input":"2025-05-12T10:03:22.135177Z","iopub.status.idle":"2025-05-12T10:03:22.140602Z","shell.execute_reply.started":"2025-05-12T10:03:22.135152Z","shell.execute_reply":"2025-05-12T10:03:22.139883Z"}},"outputs":[],"execution_count":null}]}