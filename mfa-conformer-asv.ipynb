{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5539168,"sourceType":"datasetVersion","datasetId":3192482},{"sourceId":9920101,"sourceType":"datasetVersion","datasetId":6096635},{"sourceId":11565412,"sourceType":"datasetVersion","datasetId":7251391},{"sourceId":11768589,"sourceType":"datasetVersion","datasetId":7388291},{"sourceId":11957904,"sourceType":"datasetVersion","datasetId":7494227}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchaudio soundfile pandas numpy matplotlib scikit-learn speechbrain lightning-bolts\n!pip uninstall -y typeguard\n!pip install typeguard==2.13.3  ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/zyzisyz/mfa_conformer.git","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nprint(\"Patching MFA-Conformer code for PyTorch Lightning compatibility...\")\n\nmain_py_path = \"/kaggle/working/mfa_conformer/main.py\"\n\n# --- MAIN.PY PATCH ---\nwith open(main_py_path, \"r\") as f:\n    main_content = f.read()\n\n# Replace DDPPlugin\nmain_content = main_content.replace(\n    \"from pytorch_lightning.plugins import DDPPlugin\",\n    \"from pytorch_lightning.strategies import DDPStrategy\"\n)\nmain_content = main_content.replace(\n    \"plugins=DDPPlugin(find_unused_parameters=False)\",\n    \"strategy=DDPStrategy(find_unused_parameters=False)\"\n)\n\nwith open(main_py_path, \"w\") as f:\n    f.write(main_content)\n\nprint(\"Patched main.py for PyTorch Lightning >=1.8 compatibility.\")\n\n# --- LOADER.PY PATCH ---\nloader_py_path = \"/kaggle/working/mfa_conformer/module/loader.py\"\nif os.path.exists(loader_py_path):\n    with open(loader_py_path, \"r\") as f:\n        loader_content = f.read()\n\n    if (\n        \"UnlabeledImagenet\" in loader_content\n        and \"from pl_bolts.datasets import UnlabeledImagenet\" in loader_content\n    ):\n        lines = loader_content.splitlines()\n        new_lines = []\n        for line in lines:\n            if \"from pl_bolts.datasets import UnlabeledImagenet\" in line:\n                new_lines.append(\"# \" + line + \"  # Commented out for compatibility\")\n            else:\n                new_lines.append(line)\n        with open(loader_py_path, \"w\") as f:\n            f.write('\\n'.join(new_lines))\n        print(\"Patched loader.py for pl_bolts compatibility.\")\n\nprint(\"Code patching complete.\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport argparse\nimport tqdm\nimport sys\n\ndef check_file_exists(base_dir, speaker_id, file_id):\n    \"\"\"Check if the audio file exists\"\"\"\n    full_path = os.path.join(base_dir, speaker_id, file_id)\n    return os.path.isfile(full_path)\n\ndef filter_t_file(input_file, output_file, base_dir):\n    \"\"\"Filter the training file (t) to remove lines referring to non-existent files\"\"\"\n    with open(input_file, 'r') as fin, open(output_file, 'w') as fout:\n        lines = fin.readlines()\n        print(f\"Processing {len(lines)} lines from {input_file}...\")\n        \n        kept_lines = 0\n        for line in tqdm.tqdm(lines, leave=False):\n            parts = line.strip().split()\n            if len(parts) == 2:\n                speaker_id, file_id = parts\n                if check_file_exists(base_dir, speaker_id, file_id):\n                    fout.write(line)\n                    kept_lines += 1\n        \n        print(f\"Kept {kept_lines}/{len(lines)} lines in {output_file}\")\n\ndef filter_eh_file(input_file, output_file, base_dir):\n    \"\"\"Filter the validation files (e/h) to remove lines referring to non-existent files\"\"\"\n    with open(input_file, 'r') as fin, open(output_file, 'w') as fout:\n        lines = fin.readlines()\n        print(f\"Processing {len(lines)} lines from {input_file}...\")\n        \n        kept_lines = 0\n        for line in tqdm.tqdm(lines, leave=False):\n            parts = line.strip().split()\n            if len(parts) == 3:\n                label, file1, file2 = parts\n                \n                # Split speaker_id and file_id\n                speaker_id1, file_id1 = file1.split('/')\n                speaker_id2, file_id2 = file2.split('/')\n                \n                if (check_file_exists(base_dir, speaker_id1, file_id1) and \n                    check_file_exists(base_dir, speaker_id2, file_id2)):\n                    fout.write(line)\n                    kept_lines += 1\n        \n        print(f\"Kept {kept_lines}/{len(lines)} lines in {output_file}\")\n\ndef main():\n    argv = [arg for arg in sys.argv if not arg.startswith('-f')]\n    \n    parser = argparse.ArgumentParser(description=\"Filter lines referring to non-existent files\")\n    parser.add_argument('--input_dir', type=str, default='/kaggle/input/vietnam-celeb-dataset/full-dataset',\n                        help='Directory containing Vietnam-celeb dataset')\n    parser.add_argument('--output_dir', type=str, default='/kaggle/working',\n                        help='Output directory for filtered files')\n    \n    # Parse only known args and ignore unknown ones\n    args, unknown = parser.parse_known_args(argv)\n    \n    # Paths to files and data directory\n    data_dir = os.path.join(args.input_dir, 'data')\n    t_file = os.path.join(args.input_dir, 'vietnam-celeb-t.txt')\n    e_file = os.path.join(args.input_dir, 'vietnam-celeb-e.txt')\n    h_file = os.path.join(args.input_dir, 'vietnam-celeb-h.txt')\n    \n    # Output files\n    t_output = os.path.join(args.output_dir, 'filtered-vietnam-celeb-t.txt')\n    e_output = os.path.join(args.output_dir, 'filtered-vietnam-celeb-e.txt')\n    h_output = os.path.join(args.output_dir, 'filtered-vietnam-celeb-h.txt')\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(args.output_dir, exist_ok=True)\n    \n    # Filter the files\n    print(\"Filtering training file (t)...\")\n    filter_t_file(t_file, t_output, data_dir)\n    \n    print(\"\\nFiltering validation file (e)...\")\n    filter_eh_file(e_file, e_output, data_dir)\n    \n    print(\"\\nFiltering validation file (h)...\")\n    filter_eh_file(h_file, h_output, data_dir)\n    \n    print(\"\\nAll files filtered successfully!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport argparse\nimport pandas as pd\nimport numpy as np\nimport tqdm\n\ndef create_training_csv(input_file, output_file, data_dir):\n    \"\"\"\n    Create training CSV file in MFA Conformer format:\n    - speaker_name: String label for the speaker\n    - utt_paths: Path to audio file\n    - utt_spk_int_labels: Integer label for the speaker\n    \"\"\"\n    # Read filtered training file\n    with open(input_file, 'r') as f:\n        lines = f.readlines()\n    \n    # Process the lines\n    speaker_names = []\n    utt_paths = []\n    \n    print(f\"Processing {len(lines)} lines from {input_file}...\")\n    \n    for line in tqdm.tqdm(lines, leave=False):\n        parts = line.strip().split()\n        if len(parts) == 2:\n            speaker_id, file_id = parts\n            full_path = os.path.join(data_dir, speaker_id, file_id)\n            speaker_names.append(speaker_id)\n            utt_paths.append(full_path)\n    \n    # Create speaker to integer mapping\n    unique_speakers = sorted(list(set(speaker_names)))\n    speaker_to_int = {speaker: idx for idx, speaker in enumerate(unique_speakers)}\n    \n    # Create integer labels\n    utt_spk_int_labels = [speaker_to_int[speaker] for speaker in speaker_names]\n    \n    # Create DataFrame\n    df = pd.DataFrame({\n        'speaker_name': speaker_names,\n        'utt_paths': utt_paths,\n        'utt_spk_int_labels': utt_spk_int_labels\n    })\n    \n    # Save to CSV\n    df.to_csv(output_file, index=False)\n    print(f\"Created training dataset with {len(df)} entries from {len(unique_speakers)} unique speakers\")\n    print(f\"Saved to {output_file}\")\n\ndef create_validation_file(input_files, output_file, data_dir):\n    \"\"\"\n    Create validation file in MFA Conformer format:\n    Similar to VoxCeleb trial format: label enroll_path test_path\n    Can accept a list of input files and concatenate all lines.\n    \"\"\"\n    all_lines = []\n    for input_file in input_files:\n        with open(input_file, 'r') as f:\n            lines = f.readlines()\n            all_lines.extend(lines)\n        print(f\"Loaded {len(lines)} lines from {input_file}\")\n    \n    print(f\"Processing {len(all_lines)} total lines from {input_files}...\")\n    \n    with open(output_file, 'w') as f:\n        for line in tqdm.tqdm(all_lines, leave=False):\n            parts = line.strip().split()\n            if len(parts) == 3:\n                label, file1, file2 = parts\n                \n                # Split speaker_id and file_id\n                speaker_id1, file_id1 = file1.split('/')\n                speaker_id2, file_id2 = file2.split('/')\n                \n                # Create full paths\n                enroll_path = os.path.join(data_dir, speaker_id1, file_id1)\n                test_path = os.path.join(data_dir, speaker_id2, file_id2)\n                \n                # Write in MFA Conformer validation format\n                f.write(f\"{label} {enroll_path} {test_path}\\n\")\n    \n    print(f\"Created validation file with {len(all_lines)} trials\")\n    print(f\"Saved to {output_file}\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Create MFA Conformer format datasets\")\n    parser.add_argument('--input_dir', type=str, default='/kaggle/working',\n                        help='Directory containing filtered files')\n    parser.add_argument('--output_dir', type=str, default='/kaggle/working',\n                        help='Output directory for MFA Conformer format files')\n    parser.add_argument('--data_dir', type=str, default='/kaggle/input/vietnam-celeb-dataset/full-dataset/data',\n                        help='Directory containing audio data')\n    args = parser.parse_args([])\n    \n    # Input files\n    t_input = os.path.join(args.input_dir, 'filtered-vietnam-celeb-t.txt')\n    e_input = os.path.join(args.input_dir, 'filtered-vietnam-celeb-e.txt')\n    h_input = os.path.join(args.input_dir, 'filtered-vietnam-celeb-h.txt')\n    \n    # Output files\n    train_output = os.path.join(args.output_dir, 'train.csv')\n    e_val_output = os.path.join(args.output_dir, 'validation_e.txt')\n    h_val_output = os.path.join(args.output_dir, 'validation_h.txt')\n    full_val_output = os.path.join(args.output_dir, 'validation.txt')\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(args.output_dir, exist_ok=True)\n    \n    # Create training dataset\n    print(\"Creating training dataset...\")\n    create_training_csv(t_input, train_output, args.data_dir)\n    \n    print(\"\\nCreating validation dataset from 'e' file...\")\n    create_validation_file([e_input], e_val_output, args.data_dir)\n    \n    print(\"\\nCreating validation dataset from 'h' file...\")\n    create_validation_file([h_input], h_val_output, args.data_dir)\n    \n    print(\"\\nCreating combined validation dataset from both 'e' and 'h' files...\")\n    create_validation_file([e_input, h_input], full_val_output, args.data_dir)\n    \n    print(\"\\nAll datasets created successfully!\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom scipy.io import wavfile\nimport pickle\nimport random\nfrom tqdm.notebook import tqdm\nimport gc\nimport psutil\n\n# Import augmentation tá»« mfa_conformer\nimport sys\nsys.path.append('/kaggle/working/mfa_conformer')\nfrom module.augment import WavAugment\nfrom module.dataset import load_audio\n\nclass PreAugmentedDataset:\n    \"\"\"\n    Pre-augment 60% of data vÃ  cache trong RAM\n    \"\"\"\n    def __init__(self, train_csv_path, second=3, aug_ratio=0.6, cache_dir=\"/kaggle/working/cached_data\"):\n        self.second = second\n        self.aug_ratio = aug_ratio\n        self.cache_dir = cache_dir\n        \n        # Äá»c original training data\n        df = pd.read_csv(train_csv_path)\n        self.original_labels = df[\"utt_spk_int_labels\"].values\n        self.original_paths = df[\"utt_paths\"].values\n        \n        print(f\"ðŸ“Š Original dataset: {len(self.original_paths)} samples\")\n        print(f\"ðŸ”§ Will augment {aug_ratio*100:.0f}% of data\")\n        \n        # Táº¡o cache directory\n        os.makedirs(cache_dir, exist_ok=True)\n        \n        # Initialize augmentation\n        self.wav_aug = WavAugment()\n        \n        # Storage cho cached data\n        self.cached_waveforms = []\n        self.cached_labels = []\n        self.is_augmented = []  # Track which samples are augmented\n        \n    def check_memory_usage(self):\n        \"\"\"Check current memory usage\"\"\"\n        process = psutil.Process(os.getpid())\n        memory_info = process.memory_info()\n        memory_gb = memory_info.rss / (1024**3)\n        print(f\"ðŸ’¾ Current memory usage: {memory_gb:.2f} GB\")\n        return memory_gb\n    \n    def estimate_memory_needed(self, sample_size=100):\n        \"\"\"Estimate memory needed for full dataset\"\"\"\n        print(\"ðŸ” Estimating memory requirements...\")\n        \n        # Test vá»›i sample nhá»\n        sample_indices = random.sample(range(len(self.original_paths)), min(sample_size, len(self.original_paths)))\n        total_size = 0\n        \n        for idx in sample_indices:\n            waveform = load_audio(self.original_paths[idx], self.second)\n            total_size += waveform.nbytes\n        \n        avg_size_per_sample = total_size / len(sample_indices)\n        \n        # Estimate cho full dataset (original + 60% augmented)\n        total_samples = len(self.original_paths) + int(len(self.original_paths) * self.aug_ratio)\n        estimated_memory_gb = (total_samples * avg_size_per_sample) / (1024**3)\n        \n        print(f\"ðŸ“ Average waveform size: {avg_size_per_sample/1024:.1f} KB\")\n        print(f\"ðŸ“Š Total samples (original + augmented): {total_samples}\")\n        print(f\"ðŸ’¾ Estimated memory needed: {estimated_memory_gb:.2f} GB\")\n        \n        return estimated_memory_gb\n    \n    def create_cached_dataset(self):\n        \"\"\"Pre-augment vÃ  cache toÃ n bá»™ dataset\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"ðŸš€ CREATING PRE-AUGMENTED CACHED DATASET\")\n        print(\"=\"*60)\n        \n        # Check memory\n        initial_memory = self.check_memory_usage()\n        estimated_memory = self.estimate_memory_needed()\n        \n        if estimated_memory > 12:  # Kaggle has ~13GB RAM\n            print(f\"âš ï¸  Warning: Estimated memory ({estimated_memory:.1f}GB) might exceed Kaggle limits!\")\n            print(\"ðŸ’¡ Consider reducing aug_ratio or dataset size\")\n            \n        print(f\"\\nðŸ“¥ Loading and caching data...\")\n        \n        # 1. Load táº¥t cáº£ original data\n        print(\"1ï¸âƒ£ Loading original data...\")\n        for i, (path, label) in enumerate(tqdm(zip(self.original_paths, self.original_labels), \n                                              total=len(self.original_paths), \n                                              desc=\"Loading original\")):\n            try:\n                waveform = load_audio(path, self.second)\n                self.cached_waveforms.append(waveform.copy())\n                self.cached_labels.append(label)\n                self.is_augmented.append(False)\n                \n                # Periodic memory check\n                if i % 1000 == 0 and i > 0:\n                    current_memory = self.check_memory_usage()\n                    if current_memory > 11:  # Close to limit\n                        print(f\"âš ï¸  Memory usage high: {current_memory:.1f}GB\")\n                        \n            except Exception as e:\n                print(f\"âŒ Error loading {path}: {e}\")\n                continue\n        \n        print(f\"âœ… Loaded {len(self.cached_waveforms)} original samples\")\n        \n        # 2. Create augmented versions for 60% of data\n        print(\"2ï¸âƒ£ Creating augmented data...\")\n        num_to_augment = int(len(self.original_paths) * self.aug_ratio)\n        \n        # Random select samples to augment\n        augment_indices = random.sample(range(len(self.original_paths)), num_to_augment)\n        \n        for i, orig_idx in enumerate(tqdm(augment_indices, desc=\"Creating augmented\")):\n            try:\n                # Get original waveform\n                original_waveform = self.cached_waveforms[orig_idx].copy()\n                original_label = self.cached_labels[orig_idx]\n                \n                # Apply augmentation\n                augmented_waveform = self.wav_aug(original_waveform)\n                \n                # Add to cache\n                self.cached_waveforms.append(augmented_waveform.copy())\n                self.cached_labels.append(original_label)\n                self.is_augmented.append(True)\n                \n                # Periodic memory check\n                if i % 500 == 0 and i > 0:\n                    current_memory = self.check_memory_usage()\n                    if current_memory > 11:\n                        print(f\"âš ï¸  Memory warning: {current_memory:.1f}GB\")\n                        \n            except Exception as e:\n                print(f\"âŒ Error augmenting sample {orig_idx}: {e}\")\n                continue\n        \n        # 3. Convert to more memory-efficient format\n        print(\"3ï¸âƒ£ Optimizing memory usage...\")\n        \n        # Convert lists to numpy arrays (more memory efficient)\n        print(\"   Converting to numpy arrays...\")\n        final_waveforms = []\n        final_labels = []\n        final_is_augmented = []\n        \n        for waveform, label, is_aug in zip(self.cached_waveforms, self.cached_labels, self.is_augmented):\n            if waveform is not None:\n                final_waveforms.append(waveform.astype(np.float32))  # Use float32 instead of float64\n                final_labels.append(label)\n                final_is_augmented.append(is_aug)\n        \n        # Clear original lists to free memory\n        del self.cached_waveforms, self.cached_labels, self.is_augmented\n        gc.collect()\n        \n        # Store final arrays\n        self.cached_waveforms = final_waveforms\n        self.cached_labels = np.array(final_labels)\n        self.is_augmented = np.array(final_is_augmented)\n        \n        final_memory = self.check_memory_usage()\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"âœ… CACHED DATASET CREATION COMPLETED!\")\n        print(\"=\"*60)\n        print(f\"ðŸ“Š Total samples: {len(self.cached_waveforms)}\")\n        print(f\"   - Original: {np.sum(~self.is_augmented)}\")\n        print(f\"   - Augmented: {np.sum(self.is_augmented)}\")\n        print(f\"ðŸ’¾ Memory usage: {initial_memory:.1f}GB â†’ {final_memory:.1f}GB\")\n        print(f\"ðŸš€ Ready for fast training!\")\n        \n        return True\n        \n    def save_cache(self, cache_file=\"cached_dataset.pkl\"):\n        \"\"\"Save cached dataset to disk\"\"\"\n        cache_path = os.path.join(self.cache_dir, cache_file)\n        print(f\"ðŸ’¾ Saving cached dataset to {cache_path}...\")\n        \n        cache_data = {\n            'waveforms': self.cached_waveforms,\n            'labels': self.cached_labels,\n            'is_augmented': self.is_augmented,\n            'second': self.second,\n            'aug_ratio': self.aug_ratio\n        }\n        \n        with open(cache_path, 'wb') as f:\n            pickle.dump(cache_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n        \n        print(f\"âœ… Cache saved successfully!\")\n        return cache_path\n    \n    def load_cache(self, cache_file=\"cached_dataset.pkl\"):\n        \"\"\"Load cached dataset from disk\"\"\"\n        cache_path = os.path.join(self.cache_dir, cache_file)\n        \n        if not os.path.exists(cache_path):\n            print(f\"âŒ Cache file not found: {cache_path}\")\n            return False\n        \n        print(f\"ðŸ“¥ Loading cached dataset from {cache_path}...\")\n        \n        with open(cache_path, 'rb') as f:\n            cache_data = pickle.load(f)\n        \n        self.cached_waveforms = cache_data['waveforms']\n        self.cached_labels = cache_data['labels']\n        self.is_augmented = cache_data['is_augmented']\n        self.second = cache_data['second']\n        self.aug_ratio = cache_data['aug_ratio']\n        \n        print(f\"âœ… Loaded {len(self.cached_waveforms)} cached samples\")\n        return True\n\n# Usage\ndef create_cached_training_data():\n    \"\"\"Main function Ä‘á»ƒ táº¡o cached dataset\"\"\"\n    \n    train_csv_path = \"/kaggle/working/train.csv\"\n    \n    # Check if train.csv exists\n    if not os.path.exists(train_csv_path):\n        print(f\"âŒ Training CSV not found: {train_csv_path}\")\n        return False\n    \n    # Create pre-augmented dataset\n    cached_dataset = PreAugmentedDataset(\n        train_csv_path=train_csv_path,\n        second=3,\n        aug_ratio=0.6  # 60% augmentation\n    )\n    \n    # Try to load existing cache first\n    cache_file = \"vietnam_celeb_cached_60aug.pkl\"\n    if cached_dataset.load_cache(cache_file):\n        print(\"ðŸŽ‰ Using existing cached dataset!\")\n    else:\n        print(\"ðŸ”§ Creating new cached dataset...\")\n        success = cached_dataset.create_cached_dataset()\n        if success:\n            cached_dataset.save_cache(cache_file)\n        else:\n            print(\"âŒ Failed to create cached dataset\")\n            return False\n    \n    return cached_dataset\n\n# Run the caching process\nprint(\"ðŸš€ Starting cached dataset creation...\")\ncached_dataset = create_cached_training_data()\n\nif cached_dataset:\n    print(f\"\\nðŸŽ‰ SUCCESS! Cached dataset ready with {len(cached_dataset.cached_waveforms)} samples\")\n    print(\"Next: Use this for fast training!\")\nelse:\n    print(\"âŒ Failed to create cached dataset\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nfrom sklearn.utils import shuffle\n\nclass FastCachedDataset(Dataset):\n    \"\"\"\n    Fast dataset class sá»­ dá»¥ng pre-cached data\n    \"\"\"\n    def __init__(self, cached_dataset, pairs=False, shuffle_data=True):\n        self.cached_dataset = cached_dataset\n        self.pairs = pairs\n        \n        # Get cached data\n        self.waveforms = cached_dataset.cached_waveforms\n        self.labels = cached_dataset.cached_labels\n        self.is_augmented = cached_dataset.is_augmented\n        \n        # Shuffle if requested\n        if shuffle_data:\n            indices = np.arange(len(self.waveforms))\n            np.random.shuffle(indices)\n            \n            # Apply shuffle\n            self.waveforms = [self.waveforms[i] for i in indices]\n            self.labels = self.labels[indices]\n            self.is_augmented = self.is_augmented[indices]\n        \n        print(f\"ðŸš€ FastCachedDataset initialized:\")\n        print(f\"   ðŸ“Š Total samples: {len(self.waveforms)}\")\n        print(f\"   ðŸ“ˆ Original: {np.sum(~self.is_augmented)}\")\n        print(f\"   ðŸ”Š Augmented: {np.sum(self.is_augmented)}\")\n        print(f\"   ðŸ”„ Pairs mode: {pairs}\")\n    \n    def __getitem__(self, index):\n        # Get cached waveform (already processed)\n        waveform_1 = self.waveforms[index]\n        label = self.labels[index]\n        \n        if not self.pairs:\n            return torch.FloatTensor(waveform_1), label\n        else:\n            # For pairs, return same waveform twice (or implement pair logic)\n            waveform_2 = waveform_1  # or implement custom pair logic\n            return torch.FloatTensor(waveform_1), torch.FloatTensor(waveform_2), label\n    \n    def __len__(self):\n        return len(self.waveforms)\n    \n    def get_stats(self):\n        \"\"\"Get dataset statistics\"\"\"\n        stats = {\n            'total_samples': len(self.waveforms),\n            'original_samples': np.sum(~self.is_augmented),\n            'augmented_samples': np.sum(self.is_augmented),\n            'unique_speakers': len(np.unique(self.labels)),\n            'augmentation_ratio': np.sum(self.is_augmented) / len(self.waveforms)\n        }\n        return stats\n\n# Test the fast dataset\nif 'cached_dataset' in globals() and cached_dataset is not None:\n    print(\"\\nðŸ§ª Testing FastCachedDataset...\")\n    \n    fast_dataset = FastCachedDataset(cached_dataset, pairs=False, shuffle_data=True)\n    \n    # Print stats\n    stats = fast_dataset.get_stats()\n    print(f\"\\nðŸ“Š Dataset Statistics:\")\n    for key, value in stats.items():\n        if isinstance(value, float):\n            print(f\"   {key}: {value:.3f}\")\n        else:\n            print(f\"   {key}: {value}\")\n    \n    # Test loading speed\n    print(f\"\\nâš¡ Speed test...\")\n    import time\n    \n    start_time = time.time()\n    for i in range(100):  # Test 100 samples\n        waveform, label = fast_dataset[i]\n    end_time = time.time()\n    \n    print(f\"   â±ï¸  100 samples loading time: {(end_time - start_time)*1000:.1f}ms\")\n    print(f\"   ðŸš€ Average per sample: {(end_time - start_time)*10:.1f}ms\")\n    print(f\"   ðŸ“Š Sample shape: {waveform.shape}\")\n    \n    print(\"\\nâœ… FastCachedDataset ready for training!\")\nelse:\n    print(\"âŒ Cached dataset not available. Please run the caching process first.\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport subprocess\nimport pandas as pd\nimport datetime\nimport torch\n\n# Import fast dataset class\nsys.path.append('/kaggle/working/mfa_conformer')\n\ndef create_fast_datamodule_patch():\n    \"\"\"\n    Patch MFA Conformer Ä‘á»ƒ sá»­ dá»¥ng FastCachedDataset\n    \"\"\"\n    \n    # Create new loader.py vá»›i FastCachedDataset\n    fast_loader_code = '''\nimport os\nimport numpy as np\nimport torch\nfrom pytorch_lightning import LightningDataModule\nfrom torch.utils.data import DataLoader\n\nclass FastCachedDataset:\n    \"\"\"Fast dataset from cached data\"\"\"\n    def __init__(self, cached_dataset, pairs=False):\n        self.waveforms = cached_dataset.cached_waveforms\n        self.labels = cached_dataset.cached_labels\n        self.pairs = pairs\n        \n        # Shuffle data\n        indices = np.arange(len(self.waveforms))\n        np.random.shuffle(indices)\n        self.waveforms = [self.waveforms[i] for i in indices]\n        self.labels = self.labels[indices]\n        \n        print(f\"FastCachedDataset: {len(self.waveforms)} samples loaded\")\n    \n    def __getitem__(self, index):\n        waveform_1 = self.waveforms[index]\n        label = self.labels[index]\n        \n        if not self.pairs:\n            return torch.FloatTensor(waveform_1), label\n        else:\n            return torch.FloatTensor(waveform_1), torch.FloatTensor(waveform_1), label\n    \n    def __len__(self):\n        return len(self.waveforms)\n\nclass FastSPK_datamodule(LightningDataModule):\n    def __init__(self, cached_dataset, trial_path, num_workers=4, batch_size=32, pairs=False, **kwargs):\n        super().__init__()\n        self.cached_dataset = cached_dataset\n        self.trial_path = trial_path\n        self.num_workers = num_workers\n        self.batch_size = batch_size\n        self.pairs = pairs\n        \n    def train_dataloader(self):\n        train_dataset = FastCachedDataset(self.cached_dataset, self.pairs)\n        loader = torch.utils.data.DataLoader(\n            train_dataset,\n            shuffle=True,\n            num_workers=self.num_workers,\n            batch_size=self.batch_size,\n            pin_memory=True,\n            drop_last=False,\n        )\n        return loader\n    \n    def val_dataloader(self):\n        # Import original evaluation dataset\n        from .dataset import Evaluation_Dataset\n        \n        trials = np.loadtxt(self.trial_path, str)\n        eval_path = np.unique(np.concatenate((trials.T[1], trials.T[2])))\n        print(\"number of evaluation: {}\".format(len(eval_path)))\n        \n        eval_dataset = Evaluation_Dataset(eval_path, second=-1)\n        loader = torch.utils.data.DataLoader(\n            eval_dataset,\n            num_workers=4,\n            shuffle=False, \n            batch_size=1\n        )\n        return loader\n    \n    def test_dataloader(self):\n        return self.val_dataloader()\n'''\n    \n    # Write to new file\n    fast_loader_path = \"/kaggle/working/mfa_conformer/module/fast_loader.py\"\n    with open(fast_loader_path, 'w') as f:\n        f.write(fast_loader_code)\n    \n    print(f\"âœ… Created fast datamodule at {fast_loader_path}\")\n    return fast_loader_path\n\ndef train_with_cached_data():\n    \"\"\"\n    Training vá»›i cached data - should be super fast!\n    \"\"\"\n    print(f\"ðŸš€ Starting FAST training with cached data at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    \n    # Check if cached dataset exists\n    if 'cached_dataset' not in globals() or cached_dataset is None:\n        print(\"âŒ Cached dataset not found! Please run the caching process first.\")\n        return False\n    \n    # Create fast datamodule\n    fast_loader_path = create_fast_datamodule_patch()\n    \n    # MFA Conformer path\n    mfa_path = \"/kaggle/working/mfa_conformer\"\n    \n    # Create modified main.py that uses FastSPK_datamodule\n    main_py_path = os.path.join(mfa_path, \"main_fast.py\")\n    \n    # Read original main.py\n    with open(os.path.join(mfa_path, \"main.py\"), 'r') as f:\n        main_content = f.read()\n    \n    # Replace datamodule import and usage\n    main_content = main_content.replace(\n        \"from module.loader import SPK_datamodule\",\n        \"from module.fast_loader import FastSPK_datamodule\"\n    )\n    \n    # Replace datamodule creation\n    old_dm_creation = '''dm = SPK_datamodule(train_csv_path=args.train_csv_path, trial_path=args.trial_path, second=args.second,\n            aug=args.aug, batch_size=args.batch_size, num_workers=args.num_workers, pairs=False)'''\n    \n    new_dm_creation = '''# Use cached dataset instead of CSV\n    import pickle\n    import sys\n    sys.path.append('/kaggle/working')\n    \n    # Get cached dataset from global scope\n    cached_dataset = globals().get('cached_dataset')\n    if cached_dataset is None:\n        raise ValueError(\"Cached dataset not found! Please run caching process first.\")\n    \n    dm = FastSPK_datamodule(cached_dataset=cached_dataset, trial_path=args.trial_path, \n                           batch_size=args.batch_size, num_workers=args.num_workers, pairs=False)'''\n    \n    main_content = main_content.replace(old_dm_creation, new_dm_creation)\n    \n    # Write modified main.py\n    with open(main_py_path, 'w') as f:\n        f.write(main_content)\n    \n    print(f\"âœ… Created fast training script: {main_py_path}\")\n    \n    # Training parameters\n    validation_path = \"/kaggle/working/validation.txt\"\n    save_dir = \"/kaggle/working/models\"\n    \n    if not os.path.exists(validation_path):\n        print(f\"âŒ Validation file not found: {validation_path}\")\n        return False\n    \n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Get number of classes\n    num_classes = len(np.unique(cached_dataset.cached_labels))\n    total_samples = len(cached_dataset.cached_waveforms)\n    \n    print(f\"ðŸ“Š Training data summary:\")\n    print(f\"   - Total samples: {total_samples}\")\n    print(f\"   - Unique speakers: {num_classes}\")\n    print(f\"   - Augmented ratio: {np.sum(cached_dataset.is_augmented)/total_samples:.1%}\")\n    \n    # Change to MFA directory\n    os.chdir(mfa_path)\n    \n    # Training command - should be much faster now!\n    command = [\n        \"python\", \"main_fast.py\",\n        \"--accelerator\", \"gpu\",\n        \"--devices\", \"1\",\n        \"--batch_size\", \"200\",  # Can use larger batch since no augmentation overhead\n        \"--num_workers\", \"6\",   # Can use more workers\n        \"--max_epochs\", \"25\",\n        \"--embedding_dim\", \"256\",\n        \"--save_dir\", save_dir,\n        \"--encoder_name\", \"conformer\",\n        \"--num_blocks\", \"6\",\n        \"--input_layer\", \"conv2d\",\n        \"--pos_enc_layer_type\", \"abs_pos\",\n        \"--loss_name\", \"amsoftmax\",\n        \"--learning_rate\", \"0.001\",\n        \"--step_size\", \"4\",\n        \"--gamma\", \"0.5\",\n        \"--weight_decay\", \"0.0000001\",\n        \"--trial_path\", validation_path,\n        \"--num_classes\", str(num_classes),\n        \"--warmup_step\", \"2000\",\n        \"--precision\", \"16\",\n        # NO --aug flag needed since data already augmented!\n    ]\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"âš¡ SUPER FAST TRAINING WITH CACHED DATA\")\n    print(\"=\"*60)\n    print(\"Command:\", \" \".join(command))\n    print(f\"ðŸ”¥ Expected time per epoch: ~2-3 minutes (vs 50 minutes before)\")\n    print(f\"ðŸ’¾ All data pre-loaded in RAM - no disk I/O during training!\")\n    print(\"=\"*60)\n    \n    try:\n        print(\"\\nðŸš€ Starting super fast training...\")\n        process = subprocess.run(command, check=True)\n        print(f\"\\nðŸŽ‰ FAST training completed! Return code: {process.returncode}\")\n        return True\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"\\nâŒ Training failed: {e.returncode}\")\n        return False\n\n# Run fast training\nif __name__ == \"__main__\":\n    success = train_with_cached_data()\n    if success:\n        print(\"\\nðŸŽ‰ðŸŽ‰ðŸŽ‰ FAST TRAINING COMPLETED! ðŸŽ‰ðŸŽ‰ðŸŽ‰\")\n        print(\"âš¡ Each epoch should now take 2-3 minutes instead of 50!\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport sys\nimport time\nimport glob\nimport matplotlib.pyplot as plt\nfrom IPython.display import FileLink\nfrom scipy.special import expit\nimport re\n\n# --------- CONFIGURATIONS ---------\n# Paths\nUSER_CHECKPOINT_PATH = '/kaggle/input/models'  # User can override this\nTEST_FILE = '/kaggle/input/voxvietnam/test_list.txt' \nAUDIO_DIR = '/kaggle/input/voxvietnam/wav/wav' \nMODELS_DIR = '/kaggle/working/models'\nBEST_OUTPUT_FILE = '/kaggle/working/Best_Predictions.txt'\nLAST_OUTPUT_FILE = '/kaggle/working/Last_Predictions.txt'\n\n# Model configurations - must match the trained model\nEMBEDDING_DIM = 256\nENCODER_NAME = 'conformer'\nNUM_BLOCKS = 6\nINPUT_LAYER = 'conv2d'\nPOS_ENC_LAYER_TYPE = 'abs_pos'\nLOSS_NAME = 'amsoftmax'  \nNUM_CLASSES = 880  \n\n# Device configuration\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n# ---------------------------------------------\n\nprint(f\"Starting inference at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\nprint(f\"User: VietCH57\")\nprint(f\"Using device: {DEVICE}\")\n\n# Add MFA Conformer path\nsys.path.append('/kaggle/working/mfa_conformer')\n\n# Import modules from MFA Conformer\ntry:\n    from main import Task\n    from module.dataset import load_audio\n    print(\"Successfully imported modules from mfa_conformer\")\nexcept ImportError as e:\n    print(f\"Error importing modules from mfa_conformer: {e}\")\n    print(\"Please check the mfa_conformer directory\")\n    raise\n\n# Create a dummy trial file\ndummy_trial_path = '/kaggle/working/dummy_trial.txt'\nwith open(dummy_trial_path, 'w') as f:\n    f.write(\"1 /path/to/dummy1 /path/to/dummy2\\n\")\n# ---------------------------------------------\n\ndef extract_embeddings(model, audio_paths, device='cuda'):\n    \"\"\"Extract embeddings from audio files\"\"\"\n    embeddings = {}\n    model.to(device)\n    model.eval()\n    \n    error_count = 0\n    \n    for path in tqdm(audio_paths, desc=\"Extracting embeddings\"):\n        try:\n            # Load audio with proper error checking\n            waveform = load_audio(path, second=-1)  # Use full audio (-1)\n            \n            # Basic checks on the waveform\n            if waveform is None or len(waveform) == 0:\n                print(f\"Warning: Empty audio for {path}\")\n                embeddings[path] = np.zeros(model.hparams.embedding_dim)\n                error_count += 1\n                continue\n                \n            # Convert to tensor and move to device\n            waveform = torch.FloatTensor(waveform).unsqueeze(0).to(device)\n            \n            # Extract embedding\n            with torch.no_grad():\n                outputs = model(waveform)\n                \n                # Handle different output types\n                if isinstance(outputs, tuple):\n                    embedding = outputs[0]  # Assume first element is embedding\n                else:\n                    embedding = outputs\n                    \n                # Convert to numpy and flatten if needed\n                embedding = embedding.cpu().numpy()\n                if embedding.ndim > 1:\n                    embedding = embedding[0]  # Get the first embedding if batch\n            \n            # Store embedding with audio path as key\n            embeddings[path] = embedding\n            \n        except Exception as e:\n            print(f\"Error processing {path}: {str(e)}\")\n            embeddings[path] = np.zeros(model.hparams.embedding_dim)\n            error_count += 1\n    \n    # Report error rate\n    if error_count > 0:\n        print(f\"Warning: {error_count}/{len(audio_paths)} files ({error_count/len(audio_paths)*100:.2f}%) failed to process properly\")\n    \n    return embeddings\n\ndef compute_score(emb1, emb2):\n    \"\"\"Compute cosine similarity between two embeddings and normalize to [0, 1]\"\"\"\n    # Normalize embeddings\n    emb1 = emb1 - np.mean(emb1)\n    emb2 = emb2 - np.mean(emb2)\n    \n    # Compute cosine similarity\n    score = np.dot(emb1, emb2)\n    denom = np.linalg.norm(emb1) * np.linalg.norm(emb2)\n    if denom > 0:\n        score = score / denom\n    else:\n        score = 0.0\n    \n    # Normalize to [0, 1]\n    score = expit(score)  # Use sigmoid to normalize\n\n    return score\n\ndef find_best_checkpoint(models_dir=MODELS_DIR):\n    \"\"\"Find the checkpoint with lowest EER value\"\"\"\n    best_ckpt = None\n    best_eer = float('inf')\n    \n    # Find all checkpoint files\n    all_ckpts = glob.glob(os.path.join(models_dir, '*.ckpt'))\n    \n    if not all_ckpts:\n        return None\n    \n    # Look for checkpoints with \"eer\" in filename\n    for ckpt in all_ckpts:\n        # Try to extract EER value using regex to handle format like \"epoch=7_cosine_eer=9.48.ckpt\"\n        match = re.search(r'eer=?(\\d+\\.\\d+)', os.path.basename(ckpt))\n        if match:\n            try:\n                eer = float(match.group(1))\n                if eer < best_eer:\n                    best_eer = eer\n                    best_ckpt = ckpt\n            except ValueError:\n                continue\n    \n    if best_ckpt:\n        print(f\"Found best checkpoint with EER = {best_eer}: {os.path.basename(best_ckpt)}\")\n    else:\n        print(f\"No checkpoint with EER found in {models_dir}\")\n    \n    return best_ckpt\n\ndef find_last_checkpoint(models_dir=MODELS_DIR):\n    \"\"\"Find the checkpoint with highest epoch number\"\"\"\n    last_ckpt = None\n    highest_epoch = -1\n    \n    # Find all checkpoint files\n    all_ckpts = glob.glob(os.path.join(models_dir, '*.ckpt'))\n    \n    if not all_ckpts:\n        return None\n    \n    # Look for epoch number in filenames\n    for ckpt in all_ckpts:\n        # Try to extract epoch number using regex\n        match = re.search(r'epoch=?(\\d+)', os.path.basename(ckpt))\n        if match:\n            try:\n                epoch = int(match.group(1))\n                if epoch > highest_epoch:\n                    highest_epoch = epoch\n                    last_ckpt = ckpt\n            except ValueError:\n                continue\n    \n    # If no epoch found in filenames, use last modified file\n    if last_ckpt is None:\n        # Sort by modification time (newest first)\n        last_ckpt = sorted(all_ckpts, key=os.path.getmtime, reverse=True)[0]\n        print(f\"No epoch number found in filenames, using most recently modified: {os.path.basename(last_ckpt)}\")\n    else:\n        print(f\"Found last checkpoint with epoch = {highest_epoch}: {os.path.basename(last_ckpt)}\")\n    \n    return last_ckpt\n\ndef run_inference(checkpoint_path, output_file):\n    \"\"\"Run inference using the specified checkpoint and save to output file\"\"\"\n    print(f\"\\n{'='*50}\")\n    print(f\"Running inference with checkpoint: {os.path.basename(checkpoint_path)}\")\n    print(f\"{'='*50}\")\n    \n    # Initialize model with all required parameters\n    print(f\"Loading model from {checkpoint_path}...\")\n    model = Task(\n        embedding_dim=EMBEDDING_DIM,\n        encoder_name=ENCODER_NAME,\n        num_blocks=NUM_BLOCKS,\n        input_layer=INPUT_LAYER,\n        pos_enc_layer_type=POS_ENC_LAYER_TYPE,\n        trial_path=dummy_trial_path,\n        loss_name=LOSS_NAME,  \n        num_classes=NUM_CLASSES,  \n        learning_rate=0.001,  \n        weight_decay=0.0000001,  \n        batch_size=100,  \n        num_workers=2,  \n        max_epochs=50,  \n    )\n\n    # Load trained weights\n    try:\n        checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n        model.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n        print(\"Model loaded successfully!\")\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return False\n\n    # Extract embeddings for all audio files\n    print(f\"Extracting embeddings using {DEVICE}...\")\n    embeddings = extract_embeddings(model, all_audio_files, device=DEVICE)\n    print(f\"Extracted embeddings for {len(embeddings)} audio files\")\n\n    # Compute similarity scores for each pair\n    print(\"Computing similarity scores...\")\n    scores = []\n    for audio1, audio2 in tqdm(audio_pairs, desc=\"Computing scores\"):\n        emb1 = embeddings[audio1]\n        emb2 = embeddings[audio2]\n        score = compute_score(emb1, emb2)\n        scores.append(score)\n\n    # Save scores to output file\n    os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)\n    with open(output_file, 'w') as f:\n        for score in scores:\n            f.write(f\"{score:.6f}\\n\")\n\n    # Analyze score distribution\n    scores_np = np.array(scores)\n    print(f\"Scores saved to {output_file}\")\n    print(f\"Number of scores: {len(scores)}\")\n    print(f\"Score range: {np.min(scores):.6f} to {np.max(scores):.6f}\")\n    print(f\"Score mean: {np.mean(scores):.6f}\")\n    print(f\"Score standard deviation: {np.std(scores):.6f}\")\n\n    # Plot score distribution\n    plt.figure(figsize=(12, 6))\n    plt.hist(scores_np, bins=50)\n    plt.title(f'Distribution of Similarity Scores\\n{os.path.basename(checkpoint_path)}')\n    plt.xlabel('Score')\n    plt.ylabel('Count')\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    \n    plot_filename = os.path.basename(output_file).split('.')[0] + '_distribution.png'\n    output_plot = os.path.join('/kaggle/working', plot_filename)\n    plt.savefig(output_plot)\n    print(f\"Score distribution plot saved to {output_plot}\")\n    plt.close()  # Close the plot to avoid displaying multiple plots\n\n    # Display sample scores\n    print(\"\\nSample of scores (first 5):\")\n    for i, score in enumerate(scores[:5]):\n        print(f\"  {i+1}: {score:.6f}\")\n        \n    return True\n\n# Check if test file exists\nif not os.path.exists(TEST_FILE):\n    print(f\"Test file not found at {TEST_FILE}\")\n    raise FileNotFoundError(f\"Test file not found at {TEST_FILE}\")\n\n# Read test file\naudio_pairs = []\nwith open(TEST_FILE, 'r') as f:\n    for line in f:\n        parts = line.strip().split()\n        if len(parts) != 2:\n            print(f\"Warning: Invalid line in test file: {line.strip()}\")\n            continue\n            \n        audio1 = os.path.join(AUDIO_DIR, parts[0])\n        audio2 = os.path.join(AUDIO_DIR, parts[1])\n        audio_pairs.append((audio1, audio2))\n\nprint(f\"Found {len(audio_pairs)} audio pairs in test file\")\n\n# Get all unique audio files\nall_audio_files = []\nfor pair in audio_pairs:\n    all_audio_files.extend(pair)\nall_audio_files = list(set(all_audio_files))\nprint(f\"Found {len(all_audio_files)} unique audio files\")\n\n# Check if audio files exist\nmissing_files = [path for path in all_audio_files if not os.path.exists(path)]\nif missing_files:\n    print(f\"Warning: {len(missing_files)} audio files not found\")\n    if len(missing_files) < 10:\n        for file in missing_files:\n            print(f\"  Missing: {file}\")\n    else:\n        for file in missing_files[:5]:\n            print(f\"  Missing: {file}\")\n        print(f\"  ... and {len(missing_files) - 5} more\")\n\n# Find checkpoints\nuser_ckpt = None\nif os.path.exists(USER_CHECKPOINT_PATH):\n    # If user provided a directory, find best checkpoint there\n    if os.path.isdir(USER_CHECKPOINT_PATH):\n        user_ckpt = find_best_checkpoint(USER_CHECKPOINT_PATH)\n    # If user provided a specific checkpoint file\n    elif USER_CHECKPOINT_PATH.endswith('.ckpt') and os.path.isfile(USER_CHECKPOINT_PATH):\n        user_ckpt = USER_CHECKPOINT_PATH\n    else:\n        print(f\"User checkpoint not found at {USER_CHECKPOINT_PATH}\")\n\n# Find best and last checkpoints in working models directory\nbest_ckpt = find_best_checkpoint()\nlast_ckpt = find_last_checkpoint()\n\n# Track which checkpoints we've already processed\nprocessed_checkpoints = set()\n\n# Process user checkpoint if provided\nif user_ckpt:\n    print(f\"\\nUsing user-specified checkpoint: {os.path.basename(user_ckpt)}\")\n    user_output_file = '/kaggle/working/User_Predictions.txt'\n    run_inference(user_ckpt, user_output_file)\n    processed_checkpoints.add(user_ckpt)\n\n# Process best checkpoint\nif best_ckpt and best_ckpt not in processed_checkpoints:\n    run_inference(best_ckpt, BEST_OUTPUT_FILE)\n    processed_checkpoints.add(best_ckpt)\nelse:\n    if best_ckpt in processed_checkpoints:\n        print(f\"\\nBest checkpoint already processed: {os.path.basename(best_ckpt)}\")\n    else:\n        print(\"\\nNo best checkpoint found.\")\n\n# Process last checkpoint\nif last_ckpt and last_ckpt not in processed_checkpoints:\n    run_inference(last_ckpt, LAST_OUTPUT_FILE)\n    processed_checkpoints.add(last_ckpt)\nelse:\n    if last_ckpt in processed_checkpoints:\n        print(f\"\\nLast checkpoint already processed: {os.path.basename(last_ckpt)}\")\n    else:\n        print(\"\\nNo last checkpoint found.\")\n\nprint(f\"\\nInference completed at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n\n# Create download links for prediction files\nprint(\"\\nPrediction files ready for download:\")\nfor output_file in [BEST_OUTPUT_FILE, LAST_OUTPUT_FILE, '/kaggle/working/User_Predictions.txt']:\n    if os.path.exists(output_file):\n        display(FileLink(output_file))","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}